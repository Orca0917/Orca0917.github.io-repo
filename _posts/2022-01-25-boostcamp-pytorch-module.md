---
title:  "[PyTorch] torch.nn: AutoGrad & Optimizer"
excerpt: "PyTorchì˜ Module, Parameter, Backwardê³¼ì •ì˜ ì†Œê°œ"

categories:
  - pytorch
tags:
  - [AI, Naver, BoostCamp, Python, Pytorch]
toc: true
toc_sticky: true
 
date: 2022-01-25 00:00:00
last_modified_at: 2022-01-25 00:00:00
---
ğŸ“Œ **ì•Œë¦½ë‹ˆë‹¤!**<br>
ì´ë²ˆì— ì‘ì„±ë˜ëŠ” ê¸€ì€ **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech**ë¥¼ ìˆ˜ê°•í•˜ë©° ì •ë¦¬í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.<br>
ì—¬ê¸°ì„œ ì¡´ì¬í•˜ëŠ” ê°•ì˜ ìë£Œì˜ ì¶œì²˜ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤/ìº í”„ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}

# torch.nn.Module

> Base class for all neural network modules.  
> Your models should also subclass this class.  
> Modules can also contain other Modules, allowing to nest them in a tree structure. - PyTorch docs.

`torch.nn.Module`ì€ ì‹ ê²½ë§ ëª¨ë“ˆì—ì„œ ì‚¬ìš©í•˜ëŠ” ê¸°ëŠ¥ë“¤ì„ ëª¨ì•„ë†“ì€ í•˜ë‚˜ì˜ ìƒìì´ë‹¤. ì´ ìƒìì—ëŠ” ë‹¤ì‹œ ë˜ ë‹¤ë¥¸ ëª¨ë“ˆì„ ë„£ì„ ìˆ˜ ìˆë‹¤. ëª¨ë“ˆì„ êµ¬ì„±í•˜ëŠ” ê²ƒì€ ì˜¨ì „íˆ ì„¤ê³„ìì˜ ëª«ì´ë©° ë„£ê³  ì‹¶ì€ ê¸°ëŠ¥ë“¤ì„ ëª¨ì•„ì„œ ë‚˜ë§Œì˜ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

```py
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))

model = Model()
```

ìœ„ëŠ” ëª¨ë“ˆì„ ìƒì†ë°›ì€ í•˜ë‚˜ì˜ ëª¨ë¸ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ì€ `__init__`ì˜ êµ¬í˜„ê³¼ `forward()`ì˜ êµ¬í˜„ì´ ìš”êµ¬ëœë‹¤. ë˜í•œ `torch.nn.Module`ì„ ìƒì†í•´ì•¼í•œë‹¤. ì´ë¥¼ ìƒì†í•¨ìœ¼ë¡œì¨ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ë‹¤.

## \_\_init\_\_
`__init__()`ì€ ìƒì„±ìë¡œì„œ í´ë˜ìŠ¤ê°€ í˜¸ì¶œë˜ëŠ” ê²½ìš° ë°”ë¡œ ì‹¤í–‰ì´ ì´ë£¨ì–´ì§€ëŠ” ë¶€ë¶„ì´ë‹¤. ì—¬ê¸°ì—ì„œëŠ” ëª¨ë¸ì—ì„œ ì‚¬ìš©ì´ ë  ëª¨ë“ˆë“¤ì„ ì •ì˜í•˜ê³  ë‚´ê°€ ì›í•˜ëŠ” ê¸°ëŠ¥ì„ ë‹´ì•„ ë„£ê±°ë‚˜ ì´ˆê¸°í™”ë¥¼ ì§„í–‰í•œë‹¤.

ì´ ìƒì„±ìì—ì„œëŠ” ë°˜ë“œì‹œ ìƒìœ„í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ë˜‘ê°™ì´ `__init__()`ì„ í•´ì•¼í•œë‹¤. ë¶€ëª¨í´ë˜ìŠ¤ì—ì„œë„ ë˜‘ê°™ì´ ì´ˆê¸°í™”ê°€ ì¼ì–´ë‚˜ê³  ë¶€ëª¨í´ë˜ìŠ¤ì˜ ì†ì„±ì„ ë°›ì•„ì˜¬ ìˆ˜ ìˆë„ë¡ í•´ì¤˜ì•¼ í•œë‹¤. ë§Œì•½ ì´ëŸ° ì´ˆê¸°í™”ê°€ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë¶€ëª¨í´ë˜ìŠ¤ì˜ ì†ì„±ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

ì°¸ê³ : [stackoverflow: Why is the super constructor necessary in PyTorch custom modules?](https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules)

```py
class LR(nn.Module):
    
    # Constructor
    def __init__(self, input_size, output_size):
        
        # Inherit from parent
        super(LR, self).__init__()
        self.test = 1
        self.linear = nn.Linear(input_size, output_size)
        
    
    # Prediction function
    def forward(self, x):
        out = self.linear(x)
        return out
```

ìœ„ì˜ ì½”ë“œì—ì„œ ë¶€ëª¨í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ì§€ ì•Šê³ , `self.linear = nn.Linear(input_size, output_size)` ê°€ ì‹¤í–‰ëœë‹¤ë©´ ë‚´ë¶€ì ìœ¼ë¡œëŠ” `__setattr__()`ë¥¼ ë¶€ë¥¸ë‹¤. í•´ë‹¹ ë¶€ë¶„ì˜ ì½”ë“œë¥¼ ì‚´í´ë³´ì.

```py
def __setattr__(self, name: str, value: Union[Tensor, 'Module']) -> None:
    # [...]
    modules = self.__dict__.get('_modules')
    if isinstance(value, Module):
        if modules is None:
            raise AttributeError("cannot assign module before Module.__init__() call")
        remove_from(self.__dict__, self._parameters, self._buffers, self._non_persistent_buffers_set)
        modules[name] = value
```

3ë²ˆì§¸ ì¤„ì„ ë³´ë©´ `_modules`ë¼ëŠ” ì†ì„±ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ëŠ”ë°, ì´ ì†ì„±ì„ ì´ˆê¸°í™”í•˜ì—¬ ê°€ì ¸ì™€ì•¼ë§Œ í•œë‹¤. í•´ë‹¹ì†ì„±ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ **AttributeError**ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ ì†ì„±(`_modules`)ì€ ë¶€ëª¨í´ë˜ìŠ¤ì˜ `__init__()`ì—ì„œ ì´ˆê¸°í™”ê°€ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì— `super().__init__` ì„ ì‹¤í–‰ì‹œí‚¤ë©´ ë‚´ê°€ í˜¸ì¶œí•œ ëª¨ë¸ì˜ í´ë˜ìŠ¤ì—ë„ í•´ë‹¹ ì†ì„±ì„ ìƒì†ë°›ì•„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ë‹¤.

ë‹¤ìŒì€, ë¶€ëª¨ì˜ `__init__` ì½”ë“œì´ë‹¤. ìµœí•˜ë‹¨ì„ ë³´ë©´ `_modules`ì˜ ì´ˆê¸°í™”ê°€ ì´ë£¨ì–´ì§€ê³  ìˆë‹¤.

```py
def __init__(self):
    """
    Initializes internal Module state, shared by both nn.Module and ScriptModule.
    """
    torch._C._log_api_usage_once("python.nn_module")

    self.training = True
    self._parameters = OrderedDict()
    self._buffers = OrderedDict()
    self._non_persistent_buffers_set = set()
    self._backward_hooks = OrderedDict()
    self._forward_hooks = OrderedDict()
    self._forward_pre_hooks = OrderedDict()
    self._state_dict_hooks = OrderedDict()
    self._load_state_dict_pre_hooks = OrderedDict()
    self._modules = OrderedDict() 
```

## forward
`forward`ëŠ” í˜¸ì¶œí•œ ëª¨ë¸ì—ì„œ ì¸ìë¥¼ ì „ë‹¬ë°›ì•„ì„œ ì—°ì‚°ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì„ êµ¬í˜„í•œë‹¤. ëª¨ë¸ì„ ìƒì„±í•˜ê³  ë§Œë“¤ì–´ì§„ ëª¨ë¸ì— ê°’ì„ ë„£ì—ˆì„ ë•Œ, `forward`ê°€ ë¶ˆëŸ¬ì ¸ì„œ ê³„ì‚°ëœë‹¤.

ë°˜ë©´ì—, `backward`ë¥¼ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ê²ƒì€ `autograd`ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ìë™ìœ¼ë¡œ ì •ì˜ê°€ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤.

<br>

# torch.nn.Parameter
ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í…ì„œë¥¼ ì €ì¥í•˜ê³  ë³´ê´€í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. **RNN**ì„ ì˜ˆì‹œë¡œ ë“¤ì–´ë³´ì. RNNì€ í•­ìƒ ë§ˆì§€ë§‰ Hidden Stateë¥¼ ì €ì¥í•˜ê³  ê·¸ Hidden Stateì™€ ìƒˆë¡œ ë“¤ì–´ì˜¨ ì…ë ¥ì— ëŒ€í•´ì„œ ë‹¤ì‹œ Hidden stateë¥¼ ë§Œë“¤ì–´ ì£¼ì—ˆë‹¤.

ì´ë ‡ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ í˜¸ì¶œì´ ë˜ë”ë¼ë„ ê·¸ ëª¨ë¸ì— ì €ì¥ì´ ë˜ëŠ” ë³€ìˆ˜ê°€ í•„ìš”í•´ì¡Œë‹¤. Parameterë¡œ ë“±ë¡ì„ í•˜ê²Œ ë˜ë©´, í•´ë‹¹ í…ì„œì˜ ì €ì¥ì´ ê°€ëŠ¥í•˜ë©° `parameters()`ì— ë“±ë¡ë˜ì–´ ì–´ë–¤ ê²ƒë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤.

```py
class MyLinear(nn.Module):
    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        self.weights = nn.Parameter(torch.randn(in_features, out_features))
        self.bias = nn.Parameter(torch.randn(out_features))
        
    def forward(self, x: Tensor):
        return x @ self.weights + self.bias

```

ê¸°ë³¸ì ìœ¼ë¡œ Parameterë¡œ ë“±ë¡ì´ ì´ë£¨ì–´ì§€ë©´, ì¶”ê°€ë¡œ `requires_grad=True` ë¼ëŠ” ì†ì„±ì´ ë¶™ëŠ”ë‹¤. ì´ëŠ” ì–´ë–¤ í•¨ìˆ˜ì— ëŒ€í•´ì„œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ì €ì¥í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.

<br>

# backward
`backward`ëŠ” ëª¨ë¸ì˜ ë ˆì´ì–´ì— ì¡´ì¬í•˜ëŠ” Parameterì˜ ë¯¸ë¶„ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ `forward`ë¥¼ í†µí•´ ì–»ëŠ” ê²°ê³¼ \\(\hat{y}\\) ì™€ ì‹¤ì œê°’ \\(y\\) ì˜ ì°¨ì´ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤.

```py
import torch

x = torch.ones(3, 3, requires_grad=True)
y = x + 3
z = 3 * (y ** 2)
o = z.mean()
o.backward()

print(x.grad())
>>> tensor([[2.6667, 2.6667, 2.6667],
            [2.6667, 2.6667, 2.6667],
            [2.6667, 2.6667, 2.6667]])
```

ì•„ë˜ëŠ” ìœ„ ì½”ë“œì˜ ë™ì‘ê³¼ì •ì„ ì„¤ëª…í•œë‹¤.

\begin{aligned}
    y &= x + 3\\\\\\
    z &= 3y^2\\\\\\
    o &= \frac{1}{9}z
\end{aligned}

$$ \frac{\partial o}{\partial z} \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} = \frac{1}{9}\times 6y\times 1$$

ë§ˆì§€ë§‰ìœ¼ë¡œ `[optimizer].step()`ë¥¼ ì‚¬ìš©í•˜ì—¬ Parameterë¥¼ ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆë‹¤.

```py
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ìƒì„±
import numpy as np

x_values = [i for i in range(11)]
x_train = np.array(x_values, dtype=np.float32)
x_train = x_train.reshape(-1, 1)

y_values = [2*i + 1 for i in x_values]
y_train = np.array(y_values, dtype=np.float32)
y_train = y_train.reshape(-1, 1)
```

```py
# ì„ í˜•íšŒê·€ ëª¨ë¸ì˜ ìƒì„±
import torch
from torch.autograd import Variable

class LinearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(LinearRegression, self).__init__()
        self.linear = torch.nn.Linear(inputSize, outputSize)
        
    def forward(self, x):
        out = self.linear(x)
        return out
```

```py
inputDim = 1
outputDim = 1
learningRate = 0.01
epochs = 1000

model = LinearRegression(inputDim, outputDim)

if torch.cuda.is_available():
    model.cuda()

criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)

for epoch in range(epochs):
    if torch.cuda.is_available():
        inputs = Variable(torch.from_numpy(x_train).cuda())
        labels = Variable(torch.from_numpy(y_train).cuda())
    else:
        inputs = Variable(torch.from_numpy(x_train))
        labels = Variable(torch.from_numpy(y_train))

    outputs = model(inputs) # y-hat ê³„ì‚°
    loss = criterion(outputs, labels) # y-hatê³¼ yì˜ ì˜¤ì°¨ ê³„ì‚° (MSE Loss)

    
    optimizer.zero_grad() # â— Gradientë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”
    loss.backward()
    optimizer.step()
    
    print('epoch {}, loss {}'.format(epoch, loss.item()))
```
```py
# ê²°ê³¼ê°’ í™•ì¸
with torch.no_grad():
    if torch.cuda.is_available():
        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()
    else:
        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()
    print(predicted)
```

## `optimizer.zero_grad()` ë¡œ ì„¤ì •í•˜ëŠ” ì´ìœ 

`optimizer.zero_grad()`ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì€, ìë™ë¯¸ë¶„ì´ ì§„í–‰ë˜ë©´ì„œ parameterì˜ gradientê°’ì´ ë‹¤ìŒì˜
ë¯¸ë¶„ì—ë„ ê°„ì„­ì„ ì£¼ê¸° ë•Œë¬¸ì—, ì´ˆê¸°í™”ë¥¼ ì§„í–‰í•˜ê³  í•™ìŠµì„ ì§„í–‰í•œë‹¤. ì—¬ê¸°ì„œ ê°„ì„­ì„ ì£¼ëŠ”ê²ƒì€ ë¯¸ë¶„ê°’ì¸ gradientë¥¼ ê³„ì„ ëˆ„ì í•˜ì—¬ ì €ì¥í•œë‹¤.

ì´ˆê¸°í™”ë¥¼ ì§„í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ëª¨ë¸ì˜ Parameterì™€ ìœ ì‚¬í•œ `Buffer`ì— ëˆ„ì ì´ ë˜ì–´ ì›í•˜ëŠ” ê²°ê³¼ê°’ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ì—°ì‚°ì€ RNNì˜ ëª¨ë¸ì—ì„œëŠ” ê³„ì† ëˆ„ì í•˜ì—¬ ì—°ì‚°ì„ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë„ì›€ì´ ëœë‹¤.

ì°¸ê³ 1: [stackoverflow: Why do we need to call zero_grad() in PyTorch?](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)

ì°¸ê³ 2: [â˜… 5. optimizer.zero_grad()ê°€ í•„ìš”í•œ ì´ìœ ](https://wikidocs.net/53560)

## ì§ì ‘ êµ¬í˜„í•˜ëŠ” backward
```py
class LR(nn.Module):
    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):
        super(LR, self).__init__()
        self.W = torch.zeros(dim, 1, dtype=torch.float) # Weight
        self.b = torch.scalar_tensor(0) # bias
        self.grad = {
            "dw": torch.zeros(dim, 1, dtype=torch.float),
            "db": torch.scalar_tensor(0)
        }
        self.lr = lr
    
    def sigmoid(self, z):
        return 1 / (1 + torch.exp(-z))
    
    def forward(self, x):
        z = x @ self.W.T + self.b
        return self.sigmoid(z)
    
    def backward(self, x, y_hat, y):
        self.grad['dw'] = (1 / x.shape[1]) * (x @ (y_hat - y).T)
        self.grad['db'] = (1 / x.shape[1]) * torch.sum(y_hat - y)

    def optimize(self):
        self.W -= self.lr * self.grad['dw']
        self.b -= self.lr * self.grad['db']        
```

<br>

# References

[ğŸ“˜ Pytorchì—ì„œëŠ” ì™œ í•­ìƒ optimizer.zero_grad()ë¥¼ í•´ì¤„ê¹Œ?](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)

[ğŸ“˜ TORCH.AUTOGRAD.FUNCTION.BACKWARD](https://pytorch.org/docs/stable/generated/torch.autograd.Function.backward.html?highlight=backward#torch.autograd.Function.backward)

[ğŸ“˜ Understanding torch.nn.Parameter](https://stackoverflow.com/questions/50935345/understanding-torch-nn-parameter)

[ğŸ“˜ Why is the super constructor necessary in PyTorch custom modules?](https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules)