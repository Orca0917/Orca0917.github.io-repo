---
title:  "[ë¶€ìŠ¤íŠ¸ìº í”„ Pre-Course] ë”¥ëŸ¬ë‹ ê¸°ì´ˆ: RNN"
excerpt: "Sequential modelì˜ ì •ì˜ì™€ ì¢…ë¥˜, RNNì— ëŒ€í•œ ì •ì˜ì™€ ì¢…ë¥˜"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Python, Math]
toc: true
toc_sticky: true
 
date: 2021-12-08
last_modified_at: 2021-12-08
---

# <span style = "color: #00adb5">Sequential Model</span>
`RNN`ì€ `Sequence Model`ì´ë‹¤. ì…ë ¥ê³¼ ì¶œë ¥ì„ ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì‹œí€€ìŠ¤ ëª¨ë¸ì€ ì—°ì†ëœ ë°ì´í„°ë¥¼ ì˜ë¯¸í•˜ëŠ”ë° ë§ˆì¹˜ í”íˆ ì ‘í•˜ëŠ” ìŒì„±, ë¹„ë””ì˜¤, í–‰ë™(ë™ì‘)ê³¼ ê°™ë‹¤. ì´ì²˜ëŸ¼ ì¼ìƒìƒí™œì—ì„œ ì ‘í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ëŠ” Sequence ë°ì´í„°ì´ë‹¤.

## Sequential Dataì²˜ë¦¬
### Naive Sequence Model
Sequential Dataë¥¼ í†µí•´ì„œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê³ ì í•  ë•Œ, ê³¼ê±°ì˜ ë°ì´í„°ê°€ ê³„ì† ìŒ“ì—¬ ë„ˆë¬´ ì»¤ì ¸ë²„ë¦´ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ë– í•œ ë‹¨ì–´ë“¤ì˜ ì—°ì†ì´ ì£¼ì–´ì¡Œì„ ë•Œ, **ê³¼ê±°ì— ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ì´ìš©**í•´ì„œ ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ ì§„í–‰ë˜ë©´, ì‹œê°„\\(t\\)ê°€ ì§„í–‰ë¨ì— ë”°ë¼ì„œ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ì ì  ì»¤ì§€ê²Œ ëœë‹¤. ì´ê²ƒì„ `Naive sequence model`ì´ë¼ê³  í•˜ë©°, ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

\begin{aligned}
    P(x_t|x_{t-1}, x_{t-2}, \dots, x_0)
\end{aligned}

### Autoregressive Model
ìœ„ì—ì„œ ê³ ë ¤í•´ì•¼í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ë„ˆë¬´ ë§ì•„ì§€ëŠ” ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•´, ê³ ì •ëœ ê¸°ê°„ì˜ ì´ì „ ë°ì´í„°ë§Œ ë³´ëŠ” ë°©ë²•ì´ ê³ ì•ˆë˜ì—ˆë‹¤. í•´ë‹¹ ëª¨ë¸ì„ `autoregressive model`ì´ë¼ê³  í•œë‹¤.

ë´ì•¼í•˜ëŠ” ê³¼ê±°ì˜ ê¸°ê°„ì„ \\(\tau\\) ë¼ê³  í‘œê¸°í•˜ë©´ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆê³ , ìˆ˜ì‹ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•œë‹¤.

\begin{aligned}
    P(x_t|x_{t-1}, x_{t-2}, \dots, x_{t-\tau})
\end{aligned}

### Markov Model
íŠ¹íˆ, \\(\tau = 1\\)ì¸ ê²½ìš°. ì¦‰, ë°”ë¡œ ì´ì „ì˜ ê³¼ê±°ë§Œì„ ì°¸ê³ í•˜ì—¬ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ `Markov model` ë˜ëŠ” `first order autogressive model`ì´ë¼ê³  ë¬´ë¥¸ë‹¤. í•˜ì§€ë§Œ ë§¤ìš° ë¹„íš¨ìœ¨ ì ì¸ ê²ƒì´, ë‹¤ìŒì²˜ëŸ¼ ë¹„ìœ  í•  ìˆ˜ ìˆë‹¤.

> ë‚´ì¼ ìˆ˜ëŠ¥ì„ ë³´ëŠ”ë°, ë‚´ì¼ì˜ ì ìˆ˜ëŠ” ì˜¤ëŠ˜ ë‚˜ì˜¨ ì‹œí—˜ì„±ì ì— ì˜ì¡´í•´ì„œ ë‚˜ì˜¨ë‹¤.

ì‹¤ì œë¡œëŠ”, ëª‡ ë…„ê°„ ê³µë¶€í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ëŠ¥ ì„±ì ì´ ê²°ì •ë˜ì§€ë§Œ, ë°”ë¡œ ì´ì „ì˜ ì„±ì ìœ¼ë¡œ ì ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë‹¤. í•˜ì§€ë§Œ, `joint distribution`ì„ í‘œí˜„í•˜ê¸°ê°€ ë§¤ìš° ê°„ë‹¨í•´ì§„ë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ë³´ì.

\begin{aligned}
    p(x_1, \dots, x_r) &= p(x_t|x_{t-1})p(x_{t-1}|x_{t-2})\dots p(x_2|x_1)p(x_1)\\\\\\
    &= \prod_{t=1}^{T}p(x_t|x_{t-1})
\end{aligned}

### Latent Autoregressive Model
ìœ„ì—ì„œ ë‚˜ì˜¨ ë¬¸ì œì ë“¤ì„ ëª¨ë‘ ì¢…í•©í•´ë³´ì
1. ì•ì„  ê³¼ê±°ë¥¼ ê³ ë ¤í•˜ê¸°ì—ëŠ” ê·¸ ë°ì´í„°ê°€ ë„ˆë¬´ ë°©ëŒ€í•˜ë‹¤
2. ê³¼ê±°ë¥¼ ë³´ëŠ” ê¸°ê°„ì„ ê³ ì •í•˜ê¸°ì—ëŠ” ê·¸ë³´ë‹¤ ë¨¼ ê³¼ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤.
3. ë§ˆì°¬ê°€ì§€ë¡œ, ë°”ë¡œ ì•ì˜ ê³¼ê±°ë§Œì„ ê³ ë ¤í•˜ë©´ ì•ì„  ê³¼ê±°ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤.

ìœ„ì˜ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ê³ ì•ˆëœ ëª¨ë¸ì´ `Latent Autoregressive Model`ì´ë‹¤. ì´ ëª¨ë¸ì€ ê³¼ê±°ì˜ ë°ì´í„°ë“¤ì„ ìš”ì•½í•´ì„œ ë³€ìˆ˜ \\(h_t\\)ì— ì €ì¥í•œë‹¤. í•´ë‹¹ ë³€ìˆ˜ë¥¼ `hidden state`ë¼ê³  ë§ì„ í•œë‹¤. ë§Œì•½, \\(h_t\\)ì— ëŒ€í•œ ê°’ì„ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ë©´, \\(h_{t-1}]\\)ì˜ ê°’ê³¼, ìƒˆë¡œ ë“¤ì–´ì˜¨ ë°ì´í„° \\(x_{t-1}\\)ì„ í†µí•´ì„œ ì—°ì‚°í•œë‹¤. ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

\begin{aligned}
    \hat{x} &= p(x_t|h_t)\\\\\\
    h_t &= g(h_{t-1}, x_{t-1})
\end{aligned}

<br>

# <span style = "color: #00adb5">Recurrent Neural Network</span>
RNNê³¼ MLPì˜ ì°¨ì´ì ì€ ìê¸° ìì‹ ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” êµ¬ì¡°ê°€ í•˜ë‚˜ ì¶”ê°€ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ í’€ë©´ ë‹¤ìŒ ì‚¬ì§„ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145337606-b5d167b4-c8ec-4e6c-9a1c-16c2018f481c.png){: .align-center}

## Short-term dependencies
ìœ„ì˜ RNNêµ¬ì¡°ì˜ ë¬¸ì œì ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì. ìœ„ì—ì„œëŠ” ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ ìš”ì•½í•´ì„œ hidden stateì€ \\(h_t\\)ì— ì €ì¥ì„ í–ˆì—ˆë‹¤. ê·¸ë¦¬ê³  hidden stateë¥¼ ë‹¤ì‹œ ë‹¤ìŒ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë˜ì—ˆë‹¤. 

ì´ëŸ°ì‹ìœ¼ë¡œ ê³„ì† ë°˜ë³µë˜ë©´, ë¨¼ ê³¼ê±°ì˜ ë°ì´í„°ëŠ” ë°˜ì˜ì´ ë˜ì§€ë§Œ, ì•„ì£¼ ì ê²Œ ë°˜ì˜ì´ ëœë‹¤. ê·¸ì— ë¹„í•´ì„œ ê°€ê¹Œìš´ ê³¼ê±°ì˜ ë°ì´í„°ëŠ” í¬ê²Œ ë°˜ì˜ì´ ëœë‹¤. ì§€ê¸ˆì˜ ë¬¸ì œë¥¼ ìˆ˜ì‹ì„ í†µí•´ì„œ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ëŠ”ì§€ ì•Œì•„ë³´ì.

![image](https://user-images.githubusercontent.com/91870042/145338097-44117a44-759c-4d67-baa6-59630359757a.png){: .align-center}

ìœ„ì˜ ì‚¬ì§„ì„ ë³´ë©´, ì²˜ìŒ ë“¤ì–´ì˜¨ ë°ì´í„° \\(h_0\\)ëŠ” \\(h_4\\)ê°€ ë˜ê¸° ìœ„í•´ì„œ 4ë²ˆì˜ `Activation Function`ì— ì˜í•´ ì—°ì‚°ë˜ì–´ì•¼ í•œë‹¤. ì´ ê³¼ì •ì—ì„œ ë¨¼ ë¯¸ë˜ì˜ ë°ì´í„° ì¼ìˆ˜ë¡, ì ê²Œ ë°˜ì˜ì´ ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì—ì„œ í™œì„±í•¨ìˆ˜ì— ë”°ë¼ ë‹¤ìŒ 2 í˜„ìƒì´ ì¼ì–´ë‚  ê²ƒì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

- `Vanishing gradient`: Activation Functionì„ `Sigmoid`í•¨ìˆ˜ë¡œ ì„¤ì •í–ˆì„ ë•Œ, ê·¸ ê²°ê³¼ ê°’ì€ `0~1.0`ì‚¬ì´ì˜ ê°’ì´ ë‚˜ì˜¨ë‹¤. ì´ëŸ¬í•œ ê°’ì´ ì¤‘ì²©ë˜ì–´ì„œ ê³±í•´ì§€ê²Œ ë˜ë©´ ê·¸ ê²°ê³¼ ê°’ì€ ê°ˆìˆ˜ë¡ ì‘ì•„ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

- `Exploding gradient`: Activation Functionì„ `ReLU`í•¨ìˆ˜ë¡œ ì„¤ì •í–ˆì„ ë•Œ, ê·¸ ê²°ê³¼ ê°’ì€ `0 ~ âˆ`ì‚¬ì´ì˜ ê°’ì´ ë‚˜ì˜¨ë‹¤. 1ì´ìƒì˜ ìˆ˜ê°€ ì¤‘ì²¨ë˜ì–´ì„œ ê³±í•´ì§ ë˜ë©´, ê·¸ ê²°ê³¼ ê°’ì€ ê°ˆìˆ˜ë¡ ë” ì»¤ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

<br>

# <span style = "color: #00adb5">LSTM: Long Short Term Memory</span>
ë¨¼ì €, ì§€ê¸ˆê¹Œì§€ì˜ RNNêµ¬ì¡°(`Vanilla RNN`)ë¥¼ ë‹¤ì‹œ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë³´ì. ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” ê³¼ê±°ì— ì—°ì‚°ë˜ì–´ ìš”ì•½ëœ ì •ë³´ \\(h_{t-1}\\)ì´ í˜„ì¬ ì‹œì ì˜ ë°ì´í„° ê²°í•©ë˜ì–´ `tanh` í™œì„±í•¨ìˆ˜ì— ì˜í•´ ë‹¤ì‹œ \\(h_t\\)ë¡œ ìš”ì•½ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 

![image](https://user-images.githubusercontent.com/91870042/145338380-d6a62ea9-d26b-46ac-a3ab-31e135c5d98e.png){: .align-center}

ì´ë²ˆì—ëŠ” LSTMì˜ êµ¬ì¡°ì— ëŒ€í•´ì„œ ë³´ì. ì´ êµ¬ì¡°ê°€ í•´ë‹¹ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.

![image](https://user-images.githubusercontent.com/91870042/145339011-c1622094-481a-4a2d-9139-f5f9cba13c29.png){: .align-center}

1. `previous cell state`: ê³¼ê±° ì‹œì  \\(0~t-1\\)ì˜ ì •ë³´ë¥¼ ìš”ì•½í•´ì„œ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì¸ Hidden Stateë¥¼ ë§í•œë‹¤. í”í•œ ë¹„ìœ ë¡œ `ì»¨ë² ì´ì–´ ë²¨íŠ¸`ì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤.

2. `Update cell state`: `Forget Gate`ì™€ `Input Gate`ë¥¼ í†µí•´ì„œ ì—°ì‚°í•œ ê²°ê³¼ ê°’ì„ ìƒˆë¡œìš´ `cell state`ë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„ì´ë‹¤.

    \begin{aligned}
        i_t &= \sigma(W_i \times [h_{t-1}, x_t] + b_i)\\\\\\
        C_t &= f_t * C_{t-1} + i_t * \hat{C_t}
    \end{aligned}
  

3. `Forget Gate`: ê³¼ê±°ì˜ ì •ë³´ë¥¼ ìŠê¸° ìœ„í•œ ê²Œì´íŠ¸ì´ë‹¤. ë°”ë¡œ ì´ì „ì˜ hidden stateì¸ \\(h_{t-1}\\)ê³¼ ì…ë ¥ëœ ë°ì´í„°\\(x_t\\)ë¥¼ ë°›ì•„ì„œ `Sigmoid`ë¥¼ í•´ì¤€ ê°’ì´ ì´ë²ˆ gateì˜ ì¶œë ¥ê°’ì´ë‹¤. `Sigmoid`í•¨ìˆ˜ì˜ ì¶œë ¥ ë²”ìœ„ëŠ” 0~1ì´ê¸° ë•Œë¬¸ì— ê·¸ ê°’ì´ 0ì´ë¼ë©´ ì´ì „ ìƒíƒœì˜ ì •ë³´ëŠ” ìŠê³ , 1ì´ë¼ë©´ ì´ì „ ìƒíƒœì˜ ì •ë³´ë¥¼ ê¸°ì–µí•œë‹¤.

    \begin{aligned}
        f_t = \sigma(W_f \times [h_{t-1}, x_t] + b_f)
    \end{aligned}

  
4. `Input Gate`: í˜„ì¬ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•œ ê²Œì´íŠ¸ì´ë‹¤. ë°”ë¡œ ì´ì „ì˜ hidden stateì¸ \\(h_{t-1}\\)ê³¼ ì…ë ¥ëœ ë°ì´í„°\\(x_t\\)ë¥¼ ë°›ì•„ì„œ `Sigmoid`ë¥¼ í•´ì£¼ê³ , ë˜ ë‹¤ë¥¸ ì…ë ¥ìœ¼ë¡œ, `tanh`í•´ì¤€ ê°’ì„ `Hadamard product`ì—°ì‚°ì„ ì·¨í•´ì¤€ë‹¤. ì´ ì—°ì‚° ê²°ê³¼ê°’ì´ ì´ë²ˆ ê²Œì´íŠ¸ì˜ ì¶œë ¥ ê°’ì´ë‹¤.

    \begin{aligned}
        i_t &= \sigma(W_i \times [h_{t-1}, x_t] + b_i)\\\\\\
        \hat{c_t} &= tanh(W_c \times [h_{t-1}, x_t] + b_C)
    \end{aligned}

  
5. `Output Gate`: ì–´ë–¤ ì •ë³´ë¥¼ outputìœ¼ë¡œ ì¶œë ¥í• ì§€ ê²°ì •í•˜ëŠ” ë¶€ë¶„ì´ë‹¤. ì´ ë¶€ë¶„ì— ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ëŠ” cell stateë¥¼ ë°”íƒ•ìœ¼ë¡œ í•„í„°ë§ëœ ê°’ì´ ë“¤ì–´ì˜¨ë‹¤. ì—¬ê¸°ì—ì„œ ì—°ì‚°ë˜ëŠ” ê°’ì€, \\(h_{t-1}\\)ê³¼ ì…ë ¥ëœ ë°ì´í„°\\(x_t\\)ë¥¼ ë°›ì•„ì„œ `Sigmoid`ë¥¼ í•´ì¤€ê°’ê³¼, cell stateì˜ ê°’ì„ `tanh`í•œ ê°’ì˜ ê³±ì´ë‹¤. ì´ë ‡ê²Œ ë˜ë©´, outputìœ¼ë¡œ ë³´ë‚´ê³ ì í•˜ëŠ” ë¶€ë¶„ë§Œì„ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.

    \begin{aligned}
        o_t &= \sigma(W_o \times [h_{t-1}, x_t] + b_o)\\\\\\
        h_t &= o_t * tanh(C_t)
    \end{aligned}


<br>

# <span style = "color: #00adb5">GRU: Gated Recurrent Unit</span>
LSTMì˜ ë³€í˜•ìœ¼ë¡œ, `reset gate`ì™€ `update gate` ë‹¨ 2ê°€ì§€ë§Œ ì¡´ì¬í•˜ë©°, `cell state`ê°€ ì—†ê³ , `hidden state`ë§Œ ì¡´ì¬í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145341000-fc37a481-b394-496b-99ff-aa95f9453995.png){: .align-center}

<br>

# <span style = "color: #00adb5">References</span>

[ğŸ“˜ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech 3ê¸° Pre-Course: Sequential Models - RNN](https://www.boostcourse.org/onlyboostcampaitech3/lecture/1203373?isDesc=false)

[ğŸ“˜ LSTM ì´í•´í•˜ê¸°](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)

[ğŸ“˜ RNNê³¼ LSTMì„ ì´í•´í•´ë³´ì!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)

[ğŸ“˜ Sequential Models - RNN](https://lemidia.github.io/development/boostcamp-week3-day15/)

[ğŸ“˜ Wikidocs: ìˆœí™˜ì‹ ê²½ë§](https://wikidocs.net/22886)

