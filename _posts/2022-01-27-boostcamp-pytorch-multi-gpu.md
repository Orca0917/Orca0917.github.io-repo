---
title:  "[PyTorch] Multi-GPU í•™ìŠµ"
excerpt: "PyTorchì—ì„œ Multi GPUë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ì˜ ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë³‘ë ¬í™” í•˜ëŠ” ê³¼ì •"

categories:
  - pytorch
tags:
  - [AI, Naver, BoostCamp, Python, Pytorch]
toc: true
toc_sticky: true
 
date: 2022-01-27 00:00:00
last_modified_at: 2022-01-27 00:00:00
---
ğŸ“Œ **ì•Œë¦½ë‹ˆë‹¤!**<br>
ì´ë²ˆì— ì‘ì„±ë˜ëŠ” ê¸€ì€ **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech**ë¥¼ ìˆ˜ê°•í•˜ë©° ì •ë¦¬í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.<br>
ì—¬ê¸°ì„œ ì¡´ì¬í•˜ëŠ” ê°•ì˜ ìë£Œì˜ ì¶œì²˜ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤/ìº í”„ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}

ì´ë²ˆì—ëŠ” PyTorchë¥¼ ì´ìš©í•´ì„œ Multi-GPUë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì†Œê°œí•œë‹¤. ì´ê²Œ ì¤‘ìš”í•œ ì´ìœ ëŠ” ì˜›ë‚ ì—ëŠ” ì ì€ ë°ì´í„°ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³  êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ë„ë¡ í–ˆë‹¤ë©´ ìš”ì¦˜ì—ëŠ” ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ì¡Œê³ , ë›°ì–´ë‚œ ì„±ëŠ¥ì˜ GPUê°€ ìˆì–´ ì´ë¥¼ ë§ì´ ì‚¬ìš©í•´ì„œ ë” ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ì¼ìƒí™”ê°€ ë˜ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤.

ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ì§€ê¸ˆ ë‹¹ì¥ GPUê°€ 1ê°œ ìˆë‹¤ê³  í•˜ë”ë¼ë„ ëª¨ë¸ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ê±°ë‚˜ ì •ë§ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ê²Œ ëœë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ê°œì˜ GPUë¥¼ ì‚¬ìš©í•œ Multi-GPUí•™ìŠµ ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë´ì•¼ í•œë‹¤.

# ê¸°ë³¸ Concpet
- single GPU vs multi GPU : í•˜ë‚˜ì˜ GPU, 2ê°œ ì´ìƒì˜ GPU
- GPU vs node: GPU 1ê°œ, ì»´í“¨í„°(System) 1ê°œ
- Single Node Single GPU: í•˜ë‚˜ì˜ ì»´í“¨í„°ì— í•˜ë‚˜ì˜ GPU
- Single Node Multi GPU: í•˜ë‚˜ì˜ ì»´í“¨í„°ì— ì—¬ëŸ¬ê°œì˜ GPU
- Multi Node Multi GPU: ì—¬ëŸ¬ê°œì˜ ì»´í“¨í„°ì— ì—¬ëŸ¬ê°œì˜ GPU (ì„œë²„ì‹¤)

<br>

# Model Parallel, Data Parallel

ë‹¤ì¤‘ GPUë¥¼ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì—ëŠ” 2ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤. í•˜ë‚˜ëŠ” **ëª¨ë¸ì„ ë³‘ë ¬í™”**í•˜ì—¬ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” **ë°ì´í„°ë¥¼ ë³‘ë ¬í™”**í•˜ì—¬ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.

- `Model Parallel` : ëª¨ë¸ì„ ë‚˜ëˆ ì„œ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµ
- `Data Parallel` : ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ ë³‘ë ¬ì ìœ¼ë¡œ í•™ìŠµ

![image](https://user-images.githubusercontent.com/91870042/151470401-d178e4b9-dbba-4990-8d6d-f3610d0fa7c2.png){: .align-center width="70%"}


## Model Parallel

ì—¬ëŸ¬ê°œì˜ GPUë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ í•™ìŠµì„ ê°ê° ë¶„ì‚°ì‹œí‚¤ê²Œ ë˜ì–´ íš¨ìœ¨ì„ ë” ë†’ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ëª¨ë¸ì˜ íŒŒì´í”„ë¼ì¸ì—ì„œ ë³‘ëª©í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ ë³‘ë ¬í™”ëŠ” ê³ ë‚œì´ë„ ê³¼ì œë¼ê³  í‰ê°€ë°›ê³  ìˆë‹¤.

âœ” íŒŒì´í”„ë¼ì¸(pipe line)<br>
ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì€ ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì „ì²˜ë¦¬, í•™ìŠµ ëª¨ë¸ ë°°í¬, ì˜ˆì¸¡ê¹Œì§€ì˜ ëª¨ë“  ê³¼ì •ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ëœ ë¨¸ì‹ ëŸ¬ë‹ì˜ êµ¬ì¡°ì´ë‹¤. ì´ëŠ” ê¸°ê³„í•™ìŠµ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ”ë° í•„ìš”í•œ workflowë¥¼ ì²´ê³„í™”í•˜ê³  ìë™í™”í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë‹¤ìŒì€ ML Pipelineì˜ ì˜ˆì‹œì´ë‹¤.
{: .notice--success}

![image](https://user-images.githubusercontent.com/91870042/151323432-61dff34a-2867-4d55-bb18-eb71014d76ea.png){: .align-center width="70%"}

---

ëª¨ë¸ì˜ ë³‘ë ¬í™”ë¥¼ ì§„í–‰í•  ë•Œ ìƒê¸¸ ìˆ˜ ìˆëŠ” ë¬¸ì œì ì€ GPUê°€ ê° ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ì§€ë§Œ **ì„œë¡œ ê²¹ì¹˜ì§€ ì•Šê²Œ í•™ìŠµì´ ë˜ëŠ” ê²½ìš°**ì´ë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ë³‘ë ¬í™”í•˜ëŠ” íš¨ê³¼ë¥¼ ë°›ì„ ìˆ˜ ì—†ë‹¤. ëª¨ë¸ì˜ ë³‘ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ íš¨ê³¼ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°ë¥¼ ê° GPUì—ì„œ ì²˜ë¦¬í•˜ë˜ ê²¹ì¹˜ê²Œ ì²˜ë¦¬ë¥¼ í•´ì•¼í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/151324265-d3096dbd-ebd3-4044-947d-ced1140d01c6.png){: .align-center width="70%"}

```py
class ModelParallelResNet50(ResNet):
    def __init__(self, *args, **kwargs):
        super(ModelParallelResNet50, self).__init__(
            Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs
        )

        # ì²«ë²ˆì§¸ ëª¨ë¸ì„ cuda:0 ì— í• ë‹¹
        self.seq1 = nn.Sequential(
          self.conv1, self.bn1, self.relu, self.maxpool, self.layer1, self.layer2
        ).to('cuda:0')

        # ë‘ë²ˆì§¸ ëª¨ë¸ì„ cuda:1 ì— í• ë‹¹
        self.seq2 = nn.Sequential(
          self.layer3, self.layer4, self.avgpool
        ).to('cuda:1')

        self.fc.to('cuda:1')
    
    # ë‘ ëª¨ë¸ì˜ ì—°ê²°
    def forward(self, x):
      x = self.seq2(self.seq1(x).to('cuda:1'))
      return self.fc(x.view(x.size(0), 1))
```

## Data Parallel

ë°ì´í„°ë¥¼ ë³‘ë ¬í™”í•˜ì—¬ GPUì— ë‚˜ëˆ ì„œ í• ë‹¹í•œ í›„, ê²°ê³¼ì˜ í‰ê· ì„ ì·¨í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. ë¯¸ë‹ˆë°°ì¹˜ì˜ ìˆ˜ì‹ê³¼ ìœ ì‚¬í•œì ì„ ê°–ì§€ë§Œ Data Parallelì—ì„œëŠ” í•œ ë²ˆì— ì—¬ëŸ¬ê°œì˜ GPUì—ì„œ ìˆ˜í–‰í•œë‹¤ëŠ” ì ì´ ë‹¤ë¥´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/151471838-ca3e1ead-1ec3-4d05-b043-bf02c3cff4a6.png){: .align-center}

1. Scatter mini-batch inputs to GPUs: ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ì„œ ê° GPUì—ê²Œ í• ë‹¹í•œë‹¤.
2. Replicate model on GPUs: í•™ìŠµì— ì‚¬ìš©í•  ëª¨ë¸ì„ ê° GPUì—ê²Œ ë³µì‚¬í•œë‹¤.
3. Parallel forward passes: GPUë§ˆë‹¤ ì…ë ¥ë°›ì€ ë°ì´í„°ì— ëŒ€í•´ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.
4. Gather outputs on GPU-1: 1ë²ˆ GPUì—ê²Œ ì—°ì‚°ì˜ ê²°ê³¼ë¥¼ ëª¨ë‘ í•©ì¹œë‹¤. 
5. Compute loss gradients on GPU-1: 1ë²ˆ GPUì—ì„œ ë°›ì€ ëª¨ë“  ì—°ì‚°ê²°ê³¼ì— ëŒ€í•´ì„œ lossê°’ì„ ê³„ì‚°í•œë‹¤.
6. Scatter gradients to GPUs: ì—°ì‚°ëœ lossê°’ì„ ë‹¤ì‹œ ê° GPUì—ê²Œ ëŒë ¤ì¤€ë‹¤.
7. Parallel backward passes: ê°ê°ì˜ GPUì—ì„œ ë°›ì€ lossê°’ì„ ê°€ì§€ê³  ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ê³  gradientê°’ì„ êµ¬í•œë‹¤.
8. Reduce gradients to GPU-1: GPU-1ì—ì„œ êµ¬í•´ì§„ gradient ê°’ë“¤ì„ ëª¨ë‘ ë”í•´ì„œ í‰ê· ì„ ë‚¸ë‹¤.

<br>
PyTorchì—ì„œëŠ” ìœ„ì˜ Data Parallelì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œ 2ê°€ì§€ ë°©ì‹ì„ ì œê³µí•œë‹¤.

- `DataParallel` : ë°ì´í„°ë¥¼ ë¶„ë°°í•œ í›„ í‰ê· ì„ ì·¨í•´ì„œ ê³„ì‚°í•œë‹¤. GPUì‚¬ìš©ì—ì„œ ìœ„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ 1ë²ˆ GPUê°€ í˜¼ìì„œ ë§ì´ ì¼í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë¶ˆê· í˜•ì˜ ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ batch í¬ê¸°ë¥¼ ì ê²Œë§Œë“¤ì–´ì•¼ í•œë‹¤. `GIL`ë¬¸ì œ.
- `DistributedDataParallel` : ê° CPUë§ˆë‹¤ processë¥¼ ìƒì„±í•˜ì—¬ ê°œë³„ GPUì—ê²Œ í• ë‹¹í•œë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ DataParallelì„ í•˜ì§€ë§Œ ê°œë³„ì ìœ¼ë¡œ ì—°ì‚°ì˜ í‰ê· ì„ êµ¬í•œë‹¤. (ê°ê°ì˜ GPUì—ì„œ forward, backward ëª¨ë‘ ìˆ˜í–‰.)

âœ” GIL(Global-Interface-Lock)<br>
GILì€ í•˜ë‚˜ì˜ theradë¥¼ ì‚¬ìš©í•´ì„œ Python Interpreterë¥¼ ì œì–´í•˜ëŠ” mutexì´ë‹¤. ì´ë¡œì„œ í•œë²ˆì— í•˜ë‚˜ì˜ ê¸°ë³¸ threadë§Œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ë‹¤. ì´ê²ƒì´ ìƒê¸´ ì´ìœ ëŠ” ë‹¤ë¥¸ threadë¡œ ë¶€í„° ì•ˆì „í•˜ì§€ ì•Šì€ ì½”ë“œë¥¼ ê³µìœ í•˜ëŠ” ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´ì„œ ìˆëŠ”ë°, OSì˜ mutex/lock ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.
{: .notice--success}

### DataParallel

```py
parallel_model = torch.nn.DataParallel(model)

predictions = parallel_model(inputs) # Forward
loss = loss_function(predictions, labels) # compute loss(cost)
loss.mean().backward() # average, backward
optimizer.step() # optimizer step

predictions = parallel_model(inputs) # forward
```

### DistributedDataParallel
```py
train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)

train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=False, pin_memory=True, num_workers=3, sampler=train_sampler)

from multiprocessing import Pool

def f(x):
  return x ** 2

def main():
  n_gpus = torch.cuda.device_count()
  toroch.multiprocessing.spawn(main_worker, nprocs=n_gpus, args=(n_gpus, ))

def main_worker(gpu, n_gpus):

  image_size = 224
  batch_size = 512
  num_worker = 8
  epochs = 100

  batch_size = batch_size // n_gpus
  num_worker = num_worker // n_gpus

  torch.distributed.init_process_group(
    backend='nccl', init_method='tcp://127.0.0.1:2568', world_size=n_gpus, rank=gpu
  )

  model = MODEL

  torch.cuda.set_device(gpu)
  model = model.cuda(gpu)
  model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[gpu])

if __name__ == '__main__':
  with Pool(5) as p:
    print(p.map(f, [1, 2, 3]))
```

âœ” Pin-memory<br>
ë°ì´í„°ë¥¼ CPUë¡œ ì½ì€ ë‹¤ìŒ GPUë¡œ ë³´ë‚´ê¸° ìœ„í•´ì„œëŠ” GPUì™€ í†µì‹ í•˜ê¸° ìœ„í•œ CPUì˜ ë©”ëª¨ë¦¬ ê³µê°„ì´ í•„ìš”í•˜ë‹¤. ì´ë•Œ ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹ ì‹œí‚¤ëŠ” ê¸°ë²•ì„ memory pinningì´ë¼ê³  í•œë‹¤. ì´ memory pinningì„ í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ì˜ ì¢…ë¥˜ëŠ” pinned memoryì™€ pageable memoryê°€ ìˆëŠ”ë° ì¼ë°˜ì ìœ¼ë¡œ pinned memoryê°€ ë” ë¹ ë¥´ë‹¤.
{: .notice--success}

![image](https://user-images.githubusercontent.com/91870042/151473883-06c6c98d-11f0-489b-8957-2c6f8234fb1f.png){: .align-center width="70%"}

<br>

# References

[ğŸ“˜ Global interpreter lock - Wikipedia](https://en.wikipedia.org/wiki/Global_interpreter_lock)

[ğŸ“˜ Pythonì˜ GIL](https://kimeuichan.github.io/posts/python-gil/)

[ğŸ“˜ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì´ë€? - ML Pipelineì— ëŒ€í•˜ì—¬](https://lsjsj92.tistory.com/579)

[ğŸ“˜ ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ (Machine Learning Pipeline)](http://blog.skby.net/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-machine-learning-pipeline/)

[ğŸ“˜ num_workers & pin_memory in DataLoader](https://cvml.tistory.com/24)
