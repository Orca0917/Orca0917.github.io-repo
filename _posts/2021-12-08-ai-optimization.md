---
title:  "[ë¶€ìŠ¤íŠ¸ìº í”„ Pre-Course] ë”¥ëŸ¬ë‹ ê¸°ì´ˆ: Optimization"
excerpt: "ìµœì í™”ì™€ ê´€ë ¨ëœ ì£¼ìš”í•œ ìš©ì–´ì™€ ë‹¤ì–‘í•œ Gradient Descentê¸°ë²•"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Python, Math]
toc: true
toc_sticky: true
 
date: 2021-12-08
last_modified_at: 2021-12-08
---

# <span style = "color: #00adb5">introduction</span>

> Language is the source of misunderstanings

# <span style = "color: #00adb5">Gradient Descent</span>

loss functionì„ í¸ë¯¸ë¶„ í•˜ì—¬ êµ­ì†Œì ì¸ ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ì„œ ê³„ì† ë°˜ë³¸ì ìœ¼ë¡œ ìµœì í™”ì‹œí‚¨ë‹¤.

# <span style = "color: #00adb5">Generalization</span>
ìš°ë¦¬ì˜ ëª©ì ì€ `ì¼ë°˜í™”(Generalization)`ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì¼ë°˜í™”ì˜ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤ëŠ” ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í• ê¹Œ?

![image](https://user-images.githubusercontent.com/91870042/145222399-075013f7-5baa-4fb7-bd3b-33e73faddadd.png){: .align-center}

ë¨¼ì € ë°ì´í„°ëŠ” í¬ê²Œ 2ì¢…ë¥˜ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. 
- `í•™ìŠµ ë°ì´í„°(Training Data)`: ì‹ ê²½ë§ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ ë§í•œë‹¤.
- `í…ŒìŠ¤íŠ¸ ë°ì´í„°(Test Data)`: ì‹ ê²½ë§ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íŒë‹¨í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ ë§í•œë‹¤.

ì£¼ì–´ì§„ í•™ìŠµë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ì„ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ ë˜ë©´, í•´ë‹¹ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì—ëŸ¬ëŠ” ê³„ì† ì¤„ì–´ë“ ë‹¤. í•˜ì§€ë§Œ ê·¸ë ‡ë‹¤ê³  í•™ìŠµë°ì´í„°ì— ëŒ€í•œ ì—ëŸ¬(Training Error)ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤ê³  í•´ì„œ ì¢‹ì€ ê²ƒì€ ì•„ë‹ˆë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145222399-075013f7-5baa-4fb7-bd3b-33e73faddadd.png){: .align-center}

ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ê²Œë˜ë©´, í•™ìŠµë°ì´í„°ë¡œ í•™ìŠµì„ ë§ì´ í• ìˆ˜ë¡, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ì°¨ëŠ” ì»¤ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì¼ë°˜í™”ê°€ ì¢‹ë‹¤ëŠ” ê²ƒì€ í…ŒìŠ¤íŠ¸ë°ì´í„°ì™€ í•™ìŠµë°ì´í„°ë¡œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ì˜ ì¼ê´€ì„±ì´ ì ë‹¤ëŠ” ê²ƒì´ë‹¤.

> ì¼ë°˜í™”ì˜ ì„±ëŠ¥ì´ ë†’ë‹¤ëŠ” ê²ƒì€ Train Dataì™€ Test Dataì˜ ì°¨ì´ê°€ ì ì–´, ëª¨ë‘ ì¼ê´€ì„± ìˆëŠ” ë°ì´í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.

## Overfitting & Underfitting
í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì •í™•í•˜ê²Œ ë§ì¶”ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²ƒì„ `overfitting`ì´ë¼ê³  í•œë‹¤. ê·¸ì™€ ë°˜ëŒ€ë¡œ, Neural Networkê°€ ë„ˆë¬´ ê°„ë‹¨í•˜ê±°ë‚˜, í•™ìŠµì„ ë„ˆë¬´ ì¡°ê¸ˆì‹œì¼œì„œ ì˜ ë™ì‘í•˜ì§€ ì•ŠëŠ” ê²ƒì„ `underfitting`ì´ë¼ê³  í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” ì ì ˆí•œ Neural Networkë¥¼ ìƒì„±í•˜ì—¬ ê³¼í•˜ì§€ ì•Šê²Œ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤.

## cross-validation
í•™ìŠµ ë°ì´í„°ë§Œì„ ì´ìš©í•´ì„œ í•™ìŠµì„ ì‹œì¼œ ì¢‹ì€ Nerual Networkë¥¼ ë§Œë“œëŠ”ë°ì—ëŠ” í•œê³„ê°€ ìˆë‹¤. ê·¸ë ‡ê¸°ì— ìš°ë¦¬ëŠ” ì—¬ëŸ¬ê°œì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ì¤„ ìˆ˜ ìˆëŠ” `cross-validation`ê¸°ë²•ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

Cross validationì€ í•™ìŠµë°ì´í„°ë¥¼ \\(k\\)ê°œì˜ ë¬¶ìŒìœ¼ë¡œ ë‚˜ëˆˆë‹¤. ì´ ë¬¶ìŒì¤‘ì—ì„œ ë²ˆê°ˆì•„ê°€ë©´ì„œ \\(k-1\\)ê°œì˜ ë¬¶ìŒì„ ì„ íƒí•˜ì—¬ Neural Networkë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ë‚¨ì€ 1ë¬¶ìŒì˜ í•™ìŠµë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì²˜ëŸ¼ í‰ê°€ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë‚¨ì€ 1ë¬¶ìŒì˜ í•™ìŠµë°ì´í„°ë¥¼ `validate data`ë¼ê³  í‘œí˜„í•œë‹¤.

Cross-Validationì€ `K-fold validation`ì´ë¼ í‘œí˜„í•˜ê¸°ë„ í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145223764-c6b40538-45b7-45c7-a1fc-4a69f6f3ecab.png){: .align-center}

## Bias and Variance
- `Variance`: ë°ì´í„°ë¥¼ ë„£ì—ˆì„ ë•Œ, ê²°ê³¼ê°’ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ì„±ìˆê²Œ ë‚˜ì˜¤ëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. (íƒ„ì°©êµ°)
- `Bias`: ë°ì´í„°ì˜ ê²°ê³¼ê°€ í‰ê· ì ìœ¼ë¡œ ì •ë‹µ(True Target)ì— ì ‘ê·¼í•˜ê³  ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145223996-04aa2213-3651-43b9-9749-b6c431db646d.png){: .align-center}

### Tradeoff
ë‚˜ì˜ í•™ìŠµë°ì´í„°ì— Noiseê°€ ìˆë‹¤ê³  ê°€ì •í•  ë•Œ, ì´ Noiseê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ìµœì†Œí™”ì‹œí‚¤ëŠ” ê²ƒì€ 3ê°€ì§€ êµ¬ì„±ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì•„ë˜ì—ì„œ \\(t\\)ëŠ” target dataë¥¼ ì˜ë¯¸í•˜ê³ , \\(f\\)ëŠ” neural networkë¡œ ì¶”ë¡ í•œ ê²°ê³¼ë¥¼ ë§í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145224668-d9d69a72-33c6-43d1-b74a-89e61c055334.png){: .align-center}

ë‚´ê°€ Noiseë¥¼ ìµœì†Œí™” í•˜ë ¤ê³  í•˜ëŠ” ê²ƒì€ 1ê°€ì§€ì˜ ê°’ì¸ë°, ì´ ê°’ì€ ë‹¤ì‹œ 3ê°€ì§€ (*bias, variance, noise*)ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆì–´ì„œ í•˜ë‚˜ê°€ ì¤„ì–´ë“¤ì–´ë„ ë‚˜ë¨¸ì§€ê°€ ì»¤ì§ˆ ìˆ˜ ë°–ì— ì—†ëŠ” ìƒí™©ì„ ë§í•œë‹¤.

## Bootstrapping
`Bootstrapping`ì€ ì—¬ëŸ¬ê°œì˜ ë°ì´í„°ì¤‘ ì„ì˜ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ëª¨ë“  í•™ìŠµë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ì§€ë§Œ, ì¢‹ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ë‹¤.

1. Bagging(Bootstrapping aggregating)
ì¶”ì¶œëœ ì„ì˜ì˜ í•™ìŠµë°ì´í„°ë“¤ë¡œ ì—¬ëŸ¬ê°œì˜ Neural Network Modelì„ ìƒì„±í•œë‹¤. ì´í›„, í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ê°ê°ì˜ Modelì´ ì–¼ë§ˆë‚˜ ì¼ê´€ì„±ìˆëŠ” ì¶œë ¥ì„ ê°–ëŠ”ì§€ ë¹„êµí•œë‹¤.

2. Boosting
ì¶”ì¶œëœ ì„ì˜ì˜ í•™ìŠµë°ì´í„°ë“¤ë¡œ Neural Networkë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ë‚˜ë¨¸ì§€ ë°ì´í„°ë“¤ì— ëŒ€í•´ì„œ ì˜ˆì¸¡ì„ í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°, ë‚˜ë¨¸ì§€ ë°ì´í„°ì— ëŒ€í•´ì„œ ì˜ í•™ìŠµëœ Nerual Network Modelì„ ìƒì„±í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145226008-fddb29cc-7740-4796-a652-07cb81923691.png){: .align-center}

# <span style = "color: #00adb5">Practical Gradient Descent method : ê³ ì „ì  ê²½ì‚¬í•˜ê°•ë²•</span>

- Stochastic Gradient Descent(SGD)  
í•œ ë²ˆì— í•˜ë‚˜ì˜ ë°ì´í„°ë§Œì„ ì…ë ¥í•˜ì—¬ ì¶œë ¥ê°’ì„ ì—…ë°ì´íŠ¸ í•˜ë©´ì„œ ë¹ ë¥´ê²Œ ì „ì§„í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145226572-03ea8edf-900a-4cd9-b7e7-81272db9388b.png){: .align-center}

- Mini-batch gradient descent
batch-sizeì˜ ìƒ˜í”Œì„ ì´ìš©í•´ì„œ gradientë¥¼ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸

- Batch gradient descent
í•œë²ˆì— ëª¨ë“  ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ gradientì˜ í‰ê· ì„ ì´ìš©í•´ì„œ ì—…ë°ì´íŠ¸

## Batch-size  Matters
> It has been observed in practice that when using a larger batch there is a degradation in the quality in the quality of the model, as measured by its ability to generalize.

> We ... present numerical evidence that supports the view that large batch methods tend to converge sharp minimizers of the training and testing functions. In contrase, small-batch methods consistently converge to flat minimizers... this is due to the inherent noise in the gradient estimation. - On large-batch Training for Deep Learning, 2017

ìœ„ì˜ ë§ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒ 2ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.
- `Batchí¬ê¸°ë¥¼ í¬ê²Œí•˜ëŠ” ê²½ìš°`: Sharp minimizersê°€ ëœë‹¤.
- `Batchí¬ê¸°ë¥¼ ì‘ê²Œí•˜ëŠ” ê²½ìš°`: Flat minimizersê°€ ëœë‹¤.

minimizersëŠ” Sharpë³´ë‹¤ëŠ” Flatí•œ ê²ƒì´ Generalizedê°€ ë” ì˜ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— `flat`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë”°ë¼ì„œ Batchí¬ê¸°ë¥¼ ì‘ê²Œí•˜ëŠ” ê²ƒì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ê²°ê³¼ê°’ì— ìœ ì‚¬í•˜ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145227566-7d826e05-7d59-4c7b-8af9-ebcee2e67d5c.png){: .align-center}

ìœ„ì˜ ê·¸ë¦¼ì„ ë´¤ì„ ë•Œ, Batchí¬ê¸°ë¥¼ í¬ê²Œ í•˜ì—¬ sharp minimumì— ë„ë‹¬í•˜ì˜€ì„ ê²½ìš°ì—ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ê²°ê³¼ê°’ê³¼ì˜ ì°¨ì´ê°€ ì»¤ì§„ë‹¤. (yì ˆí¸ì˜ ì°¨ì´ê°’)

# <span style = "color: #00adb5">Gradient Descent Methods</span>
## stochastic gradient descent
![image](https://user-images.githubusercontent.com/91870042/145227804-22b83532-64d4-4ba4-9a31-4d1ba56791e1.png){: .align-center}

ê³„ì‚°ëœ ê¸°ìš¸ê¸°ì— ëŒ€í•´ì„œ ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ì„œ ë‹¤ê°€ê°€ëŠ” ìˆ˜ì‹ì´ë‹¤. í•˜ì§€ë§Œ, gradientì— ëŒ€í•´ í•™ìŠµë¥ (\\(\eta\\))ì„ ì„¤ì •í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ê³¼ local minimumì— ìˆ˜ë ´í•˜ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.

## momentum
`momentum`ì€ ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ \\(W\\)ê°€ ì´ë™í•˜ëŠ” ê³¼ì •ì—ì„œ ì¼ì¢…ì˜ ê´€ì„±ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ë‹¤. \\(W\\) ê°’ì„ ì—…ë°ì´íŠ¸ í• ë•Œ, ì´ì „ì˜ ë°©í–¥ì„ ë°˜ì˜í•˜ì—¬ ê·¸ ìª½ ë°©í–¥ìœ¼ë¡œ ì§„í–‰í•œë‹¤. ì¥ì ìœ¼ë¡œ `local minimum`ì„ í”¼í•˜ê³  `global minimum`ì˜ ê°’ì— ë„ë‹¬í•˜ê²Œ í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145228766-1e722bd2-0e00-451f-bc84-df86f4665c47.png){: .align-center}

## nesterov accelerated gradient
NAGëŠ” momentumê³¼ ë¹„ìŠ·í•˜ê²Œ, ê´€ì„±ì„ ë¶€ì—¬í•´ì„œ ê²°ê³¼ê°’ì„ ì´ë™í•œë‹¤. ë‹¤ì‹œ ê·¸ ìœ„ì¹˜ì—ì„œ gradientê°’ì„ ê³„ì‚°í•´ì„œ ë‹¤ìŒ \\(W\\)ê°’ì„ ì—…ë°ì´íŠ¸ í•œë‹¤. **í•œ ë²ˆ ì´ë™í•œ ì§€ì ì—ì„œ ë‹¤ì‹œ Gradientì—°ì‚° ìˆ˜í–‰**

![image](https://user-images.githubusercontent.com/91870042/145229297-774c8b4f-adbd-4441-9246-5929aa7e00f1.png){: .align-center}

![image](https://user-images.githubusercontent.com/91870042/145229445-dea1fa3c-0ca7-4210-824f-43f326c9e8da.png){: .align-center}

## Adagrad
ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ë°ì´í„°ì˜ í•™ìŠµë¥ \\(\eta\\)ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë‹¤. í•™ìŠµë¥ ì„ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ê²Œí•˜ë©´ í•™ìŠµì´ ì•ˆë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ°ì ì„ ë³´ì™„í•´ì„œ ë§Œë“  ê²ƒì´ `Adagrad(Adaptive Gradient)`ì´ë‹¤.

í•™ìŠµë¥ ì˜ ì¡°ì •ì€ ì§€ê¸ˆê¹Œì§€ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì€ ìµœì €ì¹˜ì— ê·¼ì ‘í–ˆì„ í™•ë¥ ì´ ë†’ì„ ê²ƒìœ¼ë¡œ ë³´ê³  í•™ìŠµë¥ ì„ ì ê²Œ í•œë‹¤. ì ê²Œ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì€ ìµœì €ì¹˜ ê°’ì— ë„ë‹¬í•˜ê¸° ìœ„í•´ì„œ ë§ì´ ì´ë™í•´ì•¼í•  í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— í•™ìŠµë¥ ì„ í¬ê²Œ í•œë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ë³€í™”í•œ ìˆ˜ì¹˜ë¥¼ ì €ì¥í•˜ëŠ” ê°’ì´ í•„ìš”í•œë° \\(G_t\\)ì— ì €ì¥ë˜ì–´ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145229991-6e1e8b01-5fed-4226-bfc4-b14023a1a532.png){: .align-center}

ìœ„ì˜ ì‹ì—ì„œ \\(\epsilon\\)ëŠ” ë¶„ëª¨ê°€ 0ì´ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ê°’ì´ë‹¤. ë•Œì— ë”°ë¼ì„œëŠ” \\(\epsilon\\)ê°’ì„ ì–¼ë§Œí¼ ì„¤ì •í•˜ëŠëƒì— ë”°ë¼ì„œ í•™ìŠµì˜ ê²°ê³¼ê°€ í¬ê²Œ ë³€ë™ëœë‹¤. í•˜ì§€ë§Œ \\(G_t\\)ëŠ” ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ê·¸ ê°’ì€ ë§¤ìš° ì»¤ì§€ëŠ”ë°, ê·¸ë ‡ê²Œ ë˜ë©´ í•™ìŠµë¥  \\(\epsilon\\)ëŠ” 0ì— ê°€ê¹Œì›Œì ¸ í•™ìŠµì´ ì•ˆë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

## Adadelta
`Adadelta`ëŠ” `Adagrad`ì˜ ë¬¸ì œì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ ë‚˜ì™”ë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ AdadeltaëŠ” `ìë™ì´ë™í‰ê· `ê³¼ `ì´ì°¨ê·¼ì‚¬ë²•`ì„ ì‚¬ìš©í•œë‹¤. AdadeltaëŠ” ìœˆë„ìš° ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ê°€ í•„ìš”í•œë°, ì´ ê°’ì´ ë§¤ìš°ì»¤ì ¸ ì˜¤ë¥˜ê°€ ìƒê¸°ëŠ” ë¬¸ì œì ì´ ìˆë‹¤. Adadeltaì˜ í° íŠ¹ì§•ì€ í•™ìŠµë¥ ì„ ì§€ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145230780-a69d18f3-6477-467c-af96-ca89240e624b.png){: .align-center}

## RMSprop
Adadeltaì— ëŒ€í•´ì„œ learning rateë¥¼ ì¶”ê°€í•œ ê°œë…ì´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145231339-db6125db-5459-4cf5-9cd2-405678973315.png){: .align-center}

## Adam
Gradient í•¨ìˆ˜ì¤‘ì—ì„œ ê°€ì¥ ë¬´ë‚œí•˜ê²Œ ì‚¬ìš©ì´ ëœë‹¤. ì•ì—ì„œ ë§í•œ `Momentum`ê³¼ `Adagrad`ë¥¼ ì„ì€ í•¨ìˆ˜ì´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145231620-4ca7cf30-5240-4981-9fb9-df3228f2f140.png){: .align-center}

<br>

# <span style = "color: #00adb5">Regularization</span>
ìœ„ì—ì„œ ì•Œì•„ë³¸ `overfitting`ì´ ì¼ì–´ë‚˜ì§€ ì•Šë„ë¡, ì ì ˆí•˜ê²Œ í•™ìŠµì„ í•˜ê²Œí•˜ëŠ” ì—­í• ì„ í•œë‹¤.

## Early stopping
í•™ìŠµ ë°ì´í„°ë¥¼ ì´ìš©í•´ í•™ìŠµì„ í•˜ë‹¤ê°€ ì¼ì • ì‹œì ì´ ë˜ë©´ í•™ìŠµì„ ë©ˆì¶”ëŠ” ê²ƒì´ë‹¤. validate dataë¥¼ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ê·¸ ì°¨ì´ê°€ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ë©´, í•™ìŠµì„ ì¢…ë£Œì‹œí‚¨ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145232468-ba86e053-3748-4ede-bf0d-7b74c31a0a10.png){: .align-center}

## parameter norm penalty
parameterê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ë§Œë“œëŠ” ê²ƒì´ë‹¤. Neural Networkì˜ \\(W\\)ë¥¼ ì‘ê²Œ ë§Œë“œëŠ”ê²ƒì´ë‹¤. í•™ìŠµê³¼ì •ì—ì„œ í° ê°€ì¤‘ì¹˜ì— ëŒ€í•´ì„œëŠ” ê·¸ì— ìƒì‘í•˜ëŠ” í° í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ `overfitting`ì„ ì–µì œí•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145232696-baf725db-8076-40cb-8908-ca24a0bb7686.png){: .align-center}

## data augmentation
ë°ì´í„°ê°€ ë¬´í•œíˆ ë§ìœ¼ë©´, ì™ ë§Œí•˜ë©´ ì˜ëœë‹¤. ì–´ë–¤ì‹ìœ¼ë¡œë“  ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ì¶”ê°€ì ì¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤. ë¼ë²¨ì´ ë³€í™˜ë˜ì§€ ì•ŠëŠ” í•œì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒì‚°í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145233009-3bb3b08e-39e1-4cf5-b5ba-549b85b8ff30.png){: .align-center}

## Noise Robustness
ì…ë ¥ê³¼ weightì— ë…¸ì´ì¦ˆë¥¼ ì„ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ë” ê°•í•˜ê²Œ í•œë‹¤. ìµœì¢…ì ìœ¼ë¡œ ì´ìƒì¹˜ë‚˜ noiseê°€ ì„ì—¬ ìˆì–´ë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145233233-8b107b17-b729-4b21-8fbe-ae0b2c06bf26.png){: .align-center}


## label smoothing
í•™ìŠµë°ì´í„°ë“¤ ì¤‘ 2ê°œë¥¼ ë½‘ì•„ì„œ ë‘ ë°ì´í„°ë¥¼ ì„ëŠ” ê²ƒì´ë‹¤. ì„ëŠ” ë°©ì‹ì€ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ë“¤ì´ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145233354-3e8e56e1-9b58-4fa6-b431-c8358d628dad.png){: .align-center}


## drop out
`drop out`ì€ neural networkì˜ ì¼ë¶€ ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ ë§¤ë²ˆ ë‹¤ë¥¸ ë…¸ë“œê°€ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ì „ì²´ ê°€ì¤‘ì¹˜ê°€ overfittingë˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145233570-0636de00-8996-4cad-baf6-f5caba23bc42.png){: .align-center}

## batch normalization
ë°°ì¹˜ ì •ê·œí™”ëŠ” í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì •í•˜ëŠ” ê³¼ì •ì´ ë³„ë„ì˜ ê³¼ì •ìœ¼ë¡œ ë–¼ì–´ì§€ì§€ ì•Šê³  ì‹ ê²½ë§ ì•ˆì—ì„œ í¬í•¨ë˜ì–´ í•™ìŠµë˜ì–´ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤. Neural Networkì˜ ê° ë ˆì´ì–´ë§ˆë‹¤ ì •ê·œí™”í•˜ëŠ” ë ˆì´ì–´ë¥¼ ë‘ì–´ì„œ ë³€í˜•ëœ ë¶„í¬ê°€ ë‚˜ì˜¤ì§€ ì•Šë„ë¡ ì¡°ì ˆí•˜ëŠ” ë°©ë²•ì´ë‹¤.

ì•„ë˜ ìˆ˜ì‹ì„ ë³´ë©´, ë¯¸ë‹ˆ ë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•´ì„œ ì •ê·œí™”ë¥¼ í•œë‹¤. ì´ í›„, í•™ìŠµê°€ëŠ¥í•œ ë³€ìˆ˜ \\(\beta\\)ê°’ì„ ì´ìš©í•´ì„œ ì‹¤í–‰ëœë‹¤. (Back propagationì„ í†µí•œ í•™ìŠµ)

![image](https://user-images.githubusercontent.com/91870042/145234280-75aea743-d742-4f76-af21-44161d47c5b3.png){: .align-center}

<br>

# <span style = "color: #00adb5">References</span>

[ğŸ“˜ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech 3ê¸° Pre-Course: Optimization](https://www.boostcourse.org/onlyboostcampaitech3/lecture/1203307?isDesc=false)

[ğŸ“˜ [Deep Learning] ë°°ì¹˜ ì •ê·œí™”](https://eehoeskrap.tistory.com/430)

[ğŸ“˜ ì¸ê³µì‹ ê²½ë§ - Parameter Norm Penalies](https://munjeongkang.github.io/ANN4/)

[ğŸ“˜ ì‹ ê²½ë§ì—ì„œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë°©ë²•](https://velog.io/@yuns_u/%EA%B3%BC%EC%A0%81%ED%95%A9%EC%9D%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%9C-%EB%B0%A9%EB%B2%95Regularization-Strategies)

[ğŸ“˜ ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ê²½ì‚¬í•˜ê°•ë²• ë¹„êµ](https://www.koreascience.or.kr/article/JAKO202013261023095.pdf)
