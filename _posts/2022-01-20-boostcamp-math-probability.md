---
title: "[AI Math] 확률론"
excerpt: "확률분포, 조건부확률, 기댓값의 개념과 몬테카를로 샘플링 방법에 대한 설명"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Math]
toc: true
toc_sticky: true
 
date: 2022-01-20 00:00:00
last_modified_at: 2022-01-20 00:00:00
---
📌 **알립니다!**<br>
이번에 작성되는 글은 **네이버 부스트캠프 AI Tech**를 수강하며 정리하는 글입니다.<br>
여기서 존재하는 강의 자료의 출처는 네이버 부스트코스/캠프에게 있습니다.
{: .notice--info}

# 도입 : 딥러닝에서 확률론
딥러닝을 포함하여 기계학습의 전반적인 이론은 확률론 기반의 기계학습 이론에 바탕을 두고 있다. 기계학습에서 사용되는 손실함수(loss function)들의 작동원리가 데이터 공간을 통계적으로 해석해서 유도하는 것도 확률론에 기반을 두고 있다고 할 수 있다. 이렇게 확률론 기반으로 유도된 손실함수를 가지고 기계학습의 딥러닝 모형을 학습을 시킨다. 그렇기 때문에 확률론의 이해가 필요하다.

- 회귀 분석(regression analysis)  
이전에 살펴본 회귀분석의 손실함수로 `L2-노름`을 사용했었는데, 이는 예측오차의 분산을 가장 최소화 하는 방향으로 학습을 유도한다.
- 분류 문제(classification problem)  
분류문제에서 사용되는 교차 엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습을 하도록 유도한다.

이렇게 예측이 틀릴 위험을 최소화하도록 데이터를 학습하는 원리가 통계적 기계학습의 기본원리이며, 분산과 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야만 한다.

<br>

# 확률분포

데이터 공간을 \\(X \times Y\\)라 하고, \\(D\\)는 데이터 공간에서 데이터를 추출하는 분포이다. 이는 항상 입력 \\(X\\)에 대해서 정답레이블이 있기 때문에 \\(X \times Y\\) 라고 표기를 하는 것이고, 정답레이블 없는 경우에는 1차원 공간 \\(X\\)로 정의하게 된다. 

데이터만을 가지고 확률분포 \\(D\\) 를 알아내는 것은 불가능하기 때문에 어떤 기계학습 모형을 사용하여 우리는 확률분포 \\(D\\) 를 추론해야만 한다. 데이터는 확률변수로 \\((x, y) ~ D\\) 라고 표기한다. 여기서 \\((x, y)\\) 는 데이터 공간 \\(X \times Y\\) 위에서 관측할 수 있는 데이터를 말하며 이 데이터를 추출할 때 확률변수를 사용한다.

  ## 확률변수
  확률변수는 표본공간 내에 있는 각 원소에 하나의 실수값을 대응시키는 함수를 말한다. 이 확률변수는 확률분포 \\(D\\) 에 따라서 다음 2가지로 분류가 가능하다.

  - 이산확률변수
    이산확률변수는 확률변수가 취할 수 있는 값이 유한하거나 또는 무한히 많더라도 하나씩 셀 수 있는 경우를 말한다. 이산확률변수는 확률변수가 가질 수 있는 모든 경우의 수를 고려하여 확률을 더하여 모델링을 진행한다. 

    데이터 공간이 정수집합이라고 한다면 이산확률변수라고 생각할 수 있지만, 그렇다고 실수공간에서 정의된 데이터라고 해서 무조건 연속확률변수인 것은 아니다. 이산형인지, 연속형인지 구분하는 기준은 확률분포라는 것을 명심해야한다. 위의 수식에서 모든 원소 \\(x\\)를 가질 확률들의 총합을 \\(x\\)의 확률변수로 표현한다. 여기서는 뒤의 항을 `확률질량함수`라고 부르며 확률변수 \\(X\\) 가 \\(x\\) 를 가질 확률로 해석하는 것이 가능하다.

    $$ \mathbb{P}(X\in A)=\sum_{x\in A}P(X=x) $$

  - 연속확률변수
    확률변수가 연속적인 구간 내의 값을 취하는 경우를 말한다. 예시로 높이, 무게, 온도, 거리, 수명등의 측정자료들이 존재한다. 연속확률변수는 데이터 공간에 정의된 확률변수의 밀도위에서의 적분을 통해 모델링한다. 적분을 하는 함수 \\(P(x)\\)를 확률밀도함수라고 부르며, 밀도는 누적확률분포의 변화율을 모델링하며 확률로 해석되어서는 안된다.

    $$ \mathbb{P}(X\in A)= \int_{A}P(x)dx $$

다시 돌아와서, 다음 그림을 보면, 결합분포 \\(P(x, y)\\)는 데이터 공간을 모델링하는 것을 볼 수 있다.

![image](https://user-images.githubusercontent.com/91870042/150520294-1b900f61-a835-41df-b9e2-66b8c10ce18d.png){: .align-center}

점들이 찍힌 모습을 보면, 연속확률분포처럼 생각할 수 있지만, 빨간색으로 칸을 나누어서 이산확률분포로 생각할 수 있다. 따라서 각각의 칸에 대해서 속한 점의 개수를 세는 것이 가능해진다. 주어진 데이터의 결합분포를 이용하면 원래 확률분포 \\(D\\) 를 모델링을 하는 것이 가능하다. 이제 위에서 구한 이산확률분포를 이용하여 주변확률분포를 사용할 것이다.

> 주변확률분포?  
> 
> 두 이산확률 변수 \\(X, Y\\)의 결합확률질량함수가 \\(f(x, y)\\)일 때,   
> \\(X\\)에 대한 결합확률질량함수는 모든 \\(y\\)의 결합확률질량함수를 더하여서 구한다.

![image](https://user-images.githubusercontent.com/91870042/150521747-e29ab3c7-45d7-4f8a-9297-e1148d552107.png){: .align-center}

어떠한 입력으로 \\(x\\) 가 주어졌을 때, 그 값을 출력을 기준으로 나누는 것이 아닌, 입력 \\(x\\)에 대해서만 나누는 것을 말한다. 이처럼 주변확률분포는 \\(x\\)에 대한 정보를 제공하지만, \\(y\\) 에 대한 정보를 주지는 않는다. 다음은 확률질량함수, 확률밀도함수 각각에 대해, \\(x\\)를 기준으로 하여 주변확률분포(\\(P(x)\\))를 구하는 식이다.

$$ P(x)= \sum_{y}P(x, y)\qquad P(x)=\int_{Y}P(x, y)dy $$

조건부확률분포 \\(P(x\|y)\\) 는 특정 클래스가 주어진 상황에서 데이터의 확률분포를 보여주며 데이터 공간에서 입력 \\(x\\)와 출력 \\(y\\) 사이의 관계를 모델링한다. 다시말해, 입력변수 \\(x\\) 에 대해서 정답이 \\(y\\)일 확률을 말한다. 하지만 연속확률분포에서는 이것을 확률로 해석해서는 안되고 밀도로 해석해야 한다. 예를 들어, 만약 위의 분포에서 \\(Y=1\\)인 경우만 뽑아내면 \\(p(X\|Y=1)\\) 이라 표현하며 다음 결과가 나온다.

![image](https://user-images.githubusercontent.com/91870042/150522031-ec9bd527-d098-4ad4-9d10-3a70732de27e.png){: .align-center}

<br>

# 조건부확률과 기계학습
- 회귀문제  
  로지스틱회귀에서 사용한 선형모델과 소프트맥스의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용한다. 회귀문제는 연속확률변수의 밀도함수를 사용하기 때문에 확률해석하는 것이 어렵다. 따라서 조건부 기대값 \\(\mathbb{E}[y|x]\\) 을 추정한다.

  > 근데 왜 조건부기대값을 사용하여 확률을 추정할까?

  회귀문제를 다루는 경우에 사용하는 손실함수는 `L2-노름`의 기대값과 일치하기 때문이다. 다시 말해, 조건부 기대값이 바로 L2-노름을 최소화하는 함수와 일치한다는 것이다.

- 분류문제  
  분류문제에서 소프트맥스[\\(\text{softmax}(W\phi+b)\\)]는 데이터 \\(x\\) 로 부터 추출된 특징패턴 \\(\phi (x)\\) 과 가중치 행렬 \\(W\\) 을 통해 조건부 확률 \\(P(y|x)\\) 를 계산한다. 이를 \\(P(y|\phi (x))\\)로 표현해도 된다.

딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴을 추출하며 특징패턴을 학습하기 위해 어떤 손실함수를 사용할지는 기계학습 문제와 모델에 의해서 결정된다.

## 기대값
확률분포가 주어지면 데이터를 분석하는데 사용가능한 여러 종류의 통계적 범함수를 계산하는 것이 가능하다. 기대값은 데이터를 대표하는 통계량이며, 흔히 평균으로 사용한다. 이 기대값은 모델링하고자 하는 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용이 될 수 있다.

### 이산확률분포

$$ \mathbb{E}_{x\sim P(x)}[f(x)] = \sum_{x\in X}f(x)P(x) $$ 

### 연속확률분포

$$ \mathbb{E}_{x\sim P(x)}[f(x)] = \int_{X}f(x)P(x)dx $$

<br>

# 몬테카를로 샘플링
기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분인다. 확률분포를 모를때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법을 이용해야한다. 몬테카를로 샘플링은 반복되는 임의의 추출을 이용해서 어떤 함수의 값을 수리적으로 근사하는 것인데 1~100개의 수에서 임의로 3개씩 뽑아 평균을 구하고 이런 과정을 수없이 반복하면, 전체 평균과 근사해진다는 것이다. 단, 샘플링은 독립적으로 이루어져야만 한다.

$$ \mathbb{E}_{x\sim P(x)}[f(x)] \approx \frac{1}{N}\sum_{i=1}^{N}f(x^{(i)}), \quad x^{(i)} \sim P(x) $$

따라서 확률분포를 몰라도 샘플링만 가능하다면 기대값을 계산하는 것이 가능하기 때문에 많은 기계학습의 응용문제에서 몬테카를로 샘플링을 이용해 기대값을 계산한다.

## 예제: 적분의 계산

다음함수를 [-1, 1]의 구간에서 `몬테카를로 샘플링`을 이용해 적분값을 계산해보자

$$ \int_{-1}^{1}e^{-x^2}dx $$

구간 [-1, 1]의 길이는 2이므로, 적분값을 2로 나누면 기대값을 계산하는 것과 같다. 그렇기 때문에 몬테카를로 샘플링의 사용이 가능하다. 구간[-1, 1]사이에서 데이터의 샘플링을 진행하여 진행하면 된다. 먼저, 적분값이 어떻게 근사하는지 수식으로 나타내면 다음과 같다.

$$ \frac{1}{2}\int_{-1}^{1}e^{-x^2}dx \approx \frac{1}{N}\sum_{i=1}^{N}f(x^{(i)}), \quad x^{(i)} \sim U(-1, 1) $$

위의 수식을 이용해서 파이썬 코드로 해결을 하면 다음과 같다. 좌변의 0.5를 곱해주는 것을 우변으로 넘겨서 2(`int_len`)를 곱해주는 것도 알 수 있다.

```py
import numpy as np

def mc_int(fun, low, high, sample_size = 100, repeat = 10):
  int_len = np.abs(high - low)
  stat = []
  for _ in range(repeat):
    x = np.random.uniform(low=low, high=high, size=sample_size)
    fun_x = fun(x)
    int_val = int_len * np.mean(fun_x)
    stat.append(int_val)
  return np.mean(stat), np.std(stat)

def f_x(x):
  return np.exp(-x**2)

print(mc_int(f_x, low=-1, high=1, sample_size=10000, repeat=100))

>>> (1.493875, 0.003913)
```