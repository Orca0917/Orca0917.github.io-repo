---
title:  "[부스트캠프 Pre-Course] AI수학: 딥러닝 학습방법"
excerpt: "신경망의 구조와 내부에서 사용되는 softmax, 활성함수, 역전파 알고리즘"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Python, Math]
toc: true
toc_sticky: true
 
date: 2021-12-08
last_modified_at: 2021-12-08
---

# <span style = "color: #00adb5">신경망: 비선형 모델</span>
선형모델은 단순한 데이터를 해석할때, 도움이 되지만 분류 문제를 풀거나, 복잡한 패턴을 가진 문제를 푸는 경우에는 선형모델만을 가지고 높은 예측을 가지는 모델을 만들기는 어렵다.

딥러닝에서는 `비선형 모델`인 `신경망(neural network) 모델`을 사용한다. 신경망은 비선형 모델이지만, 신경망을 수학적으로 분해해보면, 선형모델이 그 안에 숨겨져있고, 결과적으로 **선형모델과 비선형모델의 결합**으로 이루어져있다.

## 선형모델
전체 데이터들로 놓여있는 \\(X\\)와 데이터들을 출력으로 내주기 위한 가중치 행렬\\(W\\)가 존재한다. 이 가중치 행렬\\(W\\)는 다른 차원으로 보내주기 위한 역할을 수행한다. (행렬의 역할 중 2번 째)

![image](https://user-images.githubusercontent.com/91870042/145127932-0e1b5fbb-2a65-427f-ac3c-8fd5cbeb9a20.png){: .align-center}

### 행렬의 역할
1. 대문자 X와 같이 데이터를 모아 놓은 행렬
2. 데이터를 다른 차원의 데이터로 변환시켜주는 행렬(\\(W\\))

<br>

맨오른쪽의 \\(b\\)행렬은, `y절편`에 해당하는 행렬이다. \\(b\\)라는 행렬은 행렬이지만, 각 행들이 같은 값을 가지고 있다. 이처럼, \\(X\\)와 \\(W\\), \\(b\\)의 선형결합으로 이루어져있는 것을 선형 모델이라고 부른다.

### 선형모델의 수식적인 표현

\\(X\\)라는 데이터들을 \\(W\\)라는 가중치 행렬과 \\(b\\)라는 절편 벡터의 합으로 \\(O\\)라는 출력을 만들어 낸다. 물론 데이터 \\(x\\)가 바뀌면 결과값은 바뀌게 되며, 출력벡터의 차원은 기존 \\(X\\)의 \\(d\\)차원에서 \\(p\\)차원으로 바뀌게 된다.

지금까지의 설명을 그림으로 보면 다음과 같이 나타낼 수 있다.

![image](https://user-images.githubusercontent.com/91870042/145137377-3631bd2d-d8d0-44f5-8278-301744466987.png){: .align-center}

그림에서, \\(X\\)라는 벡터가 \\(d\\)개의 변수로 이루어져있고, \\(p\\)개의 선형모델을 만들어서 잠재변수를 설명하는 모델이라고 해석할 수 있다. 화살표가 말하는것은 가중치행렬 \\(W_{ij}\\)를 의미한다.

\\(X_{i}\\)라는 행 벡터를 \\(O\\)라는 행벡터로 연결될 때, 몇 개의 모델을 만들어야 하나면, \\(p\\)개의 모델을 만들어야 한다. 전체 데이터의 개수는 \\(d\\)개 이므로, 전체 화살표의 개수는 \\(d\times p\\)가 된다. 따라서 가중치 행렬\\(W\\)의 크기는 \\(d\times p\\)로 설정되는 것이다.

## 소프트맥스
분류 문제를 해결하기 위해 학습을 시키려면, 특별한 연인 `softmax`연산이 필요하다. `소프트맥스(softmax)함수`는 모델의 출력을 확률로 해석할 수 있게 변화해주는 연산이다. 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다.

다시 말해, **softmax함수는 특정 벡터가 어떤 클래스에 속할 확률인지를 계산해주는 역할**을 한다. 출력벡터\\(O\\)에 softmax함수를 합성하면 확률 벡터가 되므로 특정 클래스 \\(k\\)에 속할 확률로 해석할 수 있다.

![image](https://user-images.githubusercontent.com/91870042/145128781-38247e1d-f947-4f7b-86be-39fd9668ed32.png){: .align-center}

![image](https://user-images.githubusercontent.com/91870042/145134329-538c1c57-a0f8-45b0-b061-3caf1da1bf87.png){: .align-center}

```py
def softmax(vec):
    # np.max를 사용하는 이유, 너무 큰 벡터가 들어오면 overflow가 발생할 수 있어서 방지하기 위해 사용한다.
    denumerator = np.exp(vec - np.max(vec, axis = -1, keepdims = True))
    numerator = np.sum(denumerator, axis = -1, keepdims = True)
    val = denumerator / numerator
    return val

vec = np.array([[1, 2, 0], [-1, 0, 1], [-10, 0, 10]])
softmax(vec)
```
```text
array([[2.44728471e-01, 6.652409563-01, 9.00305732e-02],
       [9.00305732e-01, 2.44728471e-01, 6.65240956e-01],
       [2.06106005e-01, 4.53978686e-01, 9.99954600e-01]])
```

그러나, 추론을 하는 경우에는 출력 값에서 최대값을 가지는 `원핫벡터(one-hot)`를 사용하며 학습이 아닌 `추론`을 할 경우에는 softmax를 사용하지 않는다.

> 학습을 위해서는 softmax가 필요하지만, 추론을 위해서는 one-hot벡터를 사용한다.

softmax를 해주면, 원래 선형모델의 결과물을 마치 우리가 원하고자하는 의도로 바꿔서 해석할 수 있다.

## 활성함수
`활성함수(activation function)`은 실수값의 입력을 받아서, 실수값으로 출력을 하는 비선형 함수로서 딥러닝에서 매우 중요한 개념이다.

활성함수라는 것은, 딥러닝 관점에서 말하면 한 노드에 대해 입력값을 다음 노드에 보낼지를 결정하는 비선형 함수를 말한다. 여기서 비선형 함수를 사용하는 이유는, 반대로 선형함수를 사용했을 때, 층을 깊게 해서 얻는 이점이 없기 때문이다.

### sigmoid, tanh, ReLU 활성함수
시그모이드(sigmoid)함수나 tanh함수는 전통적으로 많이 쓰이던 활성함수지만, 딥러닝에서는 ReLU함수를 많이 사용하고 있다.

![image](https://user-images.githubusercontent.com/91870042/145129859-bc583c22-f8e2-462d-afed-115062c2c4cc.png){: .align-center}

### 신경망의 수식 분해

![image](https://user-images.githubusercontent.com/91870042/145130445-5ebfcc7c-0fbe-4467-81d0-9d1c63227470.png){: .align-center}

\\(X\\)를 input으로 \\(Z\\)벡터라는 출력을 얻게 되면, 다시 이 \\(Z\\)에 활성함수(\\(\sigma\\))를 취하게 되면, 잠재벡터 \\(H\\)를 만들 수 있다. 잠재벡터 \\(H\\)를 다시, \\(W^{(2)}\\)를 패러미터로 가지는 가중치 벡터와, 절편 벡터 \\(b^{(2)}\\)의 선형변환을 통해 `2층 신경망(2-layers)`을 얻을 수 있다.

`선형모델을 반복적으로 사용`하고, `중간에 활성함수를 사용`하는 것이 신경망의 핵심이다. 위에서 2층 신경망이라고 불리는 것은 가중치 행렬 \\(W\\)가 2번 사용되기 때문이다.

다시 위를 반복적으로 더 많이 사용하게 되면, `다층 신경망(multi-layer)`, `다층 퍼셉트론`이라고 부르며 이것이 오늘날의 사용되는 딥러닝의 기본적인 모형이다.

\\(X\\)를 입력으로 받아서 \\(W^{(1)}\\)이라는 가중치 행렬을 통해 \\(Z\\)를 얻게 되었고, 다시 행렬\\(Z\\)의 연산결과에 활성함수(\\(\sigma\\))를 씌워서 \\(H\\)라는 잠재 벡터를 얻어낸다. 활성 함수를 씌울때는 각 벡터에 개별적으로 적용이 되는 것이다. \\(H\\)라는 행렬은 \\(Z\\)라는 행렬과 모양은 동일하고, \\(H\\)의 모든 구성성분들은 \\(Z\\)의 구성성분에 따라 활성함수를 씌운 차이만 존재한다.

이 메커니즘을 반복해서 여러층으로 이루면 그것이 딥러닝이 된다.

![image](https://user-images.githubusercontent.com/91870042/145130679-451df491-35d9-476c-bc89-f5097f816377.png){: .align-center}

이런식으로 \\(L\\)번의 순차적인 신경망 계산을 `순전파(forward propagation)`이라고 부른다.

> 딥러닝에서 학습에 여러 층(multi-layer)을 사용하는 이유  

이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있다. 이것을 수학적으로 `universal approximation theorem`이라고 부른다. 하지만 이것은, 이론적으로는 보장하지만, 실제에서는 무리가 있다.  

층이 깊어지면 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀더 효율적으로 학습이 가능하다. 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은 신경망이 되어야한다. 하지만 층이 깊다고 해서 최적화가 이루어졌다고 할 수 없다. 최적화가 이루어지지 않으면, 오히려 층이 깊어져서 잘못된 결과가 나오는 경우가 있따.

<br>

# <span style = "color: #00adb5">역전파 알고리즘</span>
딥러닝은 `역전파(Back propagation)` 알고리즘을 이용하여 각 층에 사용된 *parameter* = \\({(W^{(l)}, b^{(l)})}_{l=1}^{L} \\)를 학습한다. 순전파 같은 경우 \\(X\\)라는 입력을 받아서 최종 출력까지 보낼때, 선형모델과 활성함수를 반복적으로 적용한 연산이었다.

하지만, 역전파는 `경사하강법`을 적용해서 각각의 가중치 행렬을 학습시킬 때, 각각의 가중치에 대한 *gradient* 벡터를 구해야한다.

선형모델은 경사하강법을 적용할때, 한 층에서만 적용하는 것이다. 딥러닝의 경우는 순차적으로 층별로 쌓아서 계산을 하기 때문에, 한번에 계산할 수 없고, 역전파 알고리즘을 통해 역순으로 계산해야 한다.

![image](https://user-images.githubusercontent.com/91870042/145131426-a569334c-d9c4-46f7-b269-914d5791207f.png){: .align-center}

빨간색 화살표는 위에서 연산한 *gradient* 벡터를 밑으로 전달하는 것이다. 밑에 층에서 *gradient* 벡터를 계산할 때, 위층에 있는 *gradient* 벡터가 필요하기 때문이다. 그렇기 때문에 위층부터 순서대로 하는 것이다.

> 역전파는 각 층 parameter의 gradient 벡터는 윗층부터 역순으로 계산된다.

![image](https://user-images.githubusercontent.com/91870042/145131609-72976fff-deeb-4483-860d-8f92999dbfb7.png){: .align-center}

## 합성함수의 미분
역전파 알고리즘은 `합성함수 미분법인 연쇄법칙 기반 자동미분`을 사용한다. 밑에서 예시를 보면, \\(z\\)를 \\(x\\)에 대해서 미분하는 결과를 보여준다.

![image](https://user-images.githubusercontent.com/91870042/145131895-46bad46f-2d0c-4e28-ab2f-b297176a9a79.png){: .align-center}

> 역전파는 각층의 미분의 결과값을 컴퓨터에 저장을 하고 있어야 하기 때문에, 역전파는 순전파보다 메모리를 더 많이 사용하게 된다. 

## 예제: 2층 신경망
2층 신경망은 2개의 가중치 행렬 \\(W^{(1)}\\)과 \\(W^{(2)}\\)로 이루어져 있고, 히든벡터 \\(H\\)는 활성함수\\(\sigma\\)를 통해서 \\(Z\\)에 활성함수를 씌워서 얻어낼 수 있다. 수식은 복잡하지만, 활성함수의 필요성과 역전파가 어떻게 진행되는지의 대한 원리를 알고 있으면 된다.

![image](https://user-images.githubusercontent.com/91870042/145132429-b9c8de64-4357-450f-9961-1f062a365e6b.png){: .align-center}

![image](https://user-images.githubusercontent.com/91870042/145132468-80f62368-fab4-467f-a370-6796d85baa49.png){: .align-center}

![image](https://user-images.githubusercontent.com/91870042/145132498-bcc354e0-d19b-45d3-b47c-39ce38a07f6c.png){: .align-center}

<br>

# <span style = "color: #00adb5">References</span>
[📘 부스트캠프 AI Tech 3기 Pre-Course: 딥러닝 학습방법 이해하기](https://www.boostcourse.org/onlyboostcampaitech3/lecture/1203365?isDesc=false)

[📘 딥러닝에서 사용하는 활성화 함수](https://reniew.github.io/12/)

[📘 Activation Function(활성함수)](https://blog.naver.com/good5229/221752705030)