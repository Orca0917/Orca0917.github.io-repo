---
title:  "[PyTorch] Troubleshooting"
excerpt: "PyTorchë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ìì£¼ ë°œìƒí•˜ëŠ” OOM ì—ëŸ¬ì™€ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•"

categories:
  - pytorch
tags:
  - [AI, Naver, BoostCamp, Python, Pytorch]
toc: true
toc_sticky: true
 
date: 2022-01-27 02:00:00
last_modified_at: 2022-01-27 02:00:00
---
ğŸ“Œ **ì•Œë¦½ë‹ˆë‹¤!**<br>
ì´ë²ˆì— ì‘ì„±ë˜ëŠ” ê¸€ì€ **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech**ë¥¼ ìˆ˜ê°•í•˜ë©° ì •ë¦¬í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.<br>
ì—¬ê¸°ì„œ ì¡´ì¬í•˜ëŠ” ê°•ì˜ ìë£Œì˜ ì¶œì²˜ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤/ìº í”„ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}

# OOM (Out Of Memory)
OOMì—ëŸ¬ëŠ” ì™œ ë°œìƒí–ˆëŠ”ì§€ ì•Œê¸°ê°€ ì–´ë µê³ , ì–´ë””ì„œ ë°œìƒí–ˆëŠ”ì§€ ì•Œê¸°ì–´ë µë‹¤. ë˜í•œ Error backtrakingì„ í•˜ê²Œ ë˜ë©´ ì „í˜€ ëª¨ë¥´ëŠ” ê¹Šì€ ê³³ì—ì„œ ë°œìƒí–ˆë‹¤ê³  ë‚˜ì˜¤ë©°, ë©”ëª¨ë¦¬ ì—ëŸ¬ê°€ ë°œìƒí•œ ì´ì „ìƒí™©ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì–´ë µë‹¤.

1ì°¨ì ì¸ í•´ê²°ë°©ë²•ì€ ë¨¼ì €, ë°°ì¹˜í¬ê¸°ë¥¼ ì¤„ì´ê³  GPUì˜ ìºì‹œë¥¼ ì§€ìš°ê³  ë‹¤ì‹œ ì‹¤í–‰ì‹œí‚¤ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤. í•˜ì§€ë§Œ ì´ì™¸ì—ë„ ì•ˆë˜ëŠ” ê²½ìš°ì—ëŠ” í•™ìŠµí•˜ëŠ”ë° ìˆì–´ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì›ì¸ë“¤ì„ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆë‹¤.

## GPU Util(GPUtil)
GPUtilì€ nvidia-smi ì²˜ëŸ¼ GPUì˜ ìƒíƒœë¥¼ ë³´ì—¬ì£¼ëŠ” ëª¨ë“ˆì´ë©° ê° iterationë§ˆë‹¤ ë©”ëª¨ë¦¬ê°€ ëŠ˜ì–´ë‚˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìš©ì´í•˜ë‹¤. (nvidia-smiëŠ” iterationì´ ëŒë©´ì„œ ë©”ëª¨ë¦¬ê°€ ì¦ê°€í•˜ëŠ” ê²ƒì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. "í˜„ì¬ì‹œì ì˜ ìƒíƒœë§Œ ë³´ì—¬ì¤€ë‹¤!")

```py
import GPUtil
GPUtil.showUtilization()

>>> | ID | GPU | MEM |
    ------------------
    |  0 | 13% | 11% |
```

## empty_cache()
`torch.cuda.empty_cache()`ë¥¼ ì‚¬ìš©í•´ë³´ëŠ” ê²ƒì´ ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤. í•´ë‹¹ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê²Œ ë˜ë©´ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” GPUì˜ ìºì‹œë¥´ ì •ë¦¬í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•œë‹¤. `del` í‚¤ì›Œë“œëŠ” ë‹¨ìˆœíˆ ì—°ê²°ê´€ê³„ë§Œ ëŠì–´ì£¼ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ì£¼ì§€ëŠ” ì•ŠëŠ”ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, `backward()`ê°€ ì¼ì–´ë‚ ë•Œ, ë©”ëª¨ë¦¬ ë²„í¼ì— ê³„ì† ë°ì´í„°ê°€ ëˆ„ì ë˜ì–´ì„œ OOMì´ ë°œìƒí•  ìˆ˜ë„ ìˆë‹¤.

```py
import torch
from GPUtil import showUtilization as gpu_usage

print('Initial GPU Usage')
gpu_usage()

tensorList = []
for x in range(10):
    tensorList.append(torch.randn(10000000, 10).cuda())

print('GPU Usage after allocationg a bunch of Tensors')
gpu_usage()

del tensorList
print('GPU Usage after deleting the Tensors')
gpu_usage()

print('GPU Usage after emptying the cache')
torch.cuda.empty_cache()
gpu_usage()

>>> Initial GPU Usage
    | ID | GPU | MEM |
    ------------------
    |  0 |  1% | 12% |

    GPU Usage after allocationg a bunch of Tensors
    | ID | GPU | MEM |
    ------------------
    |  0 | 42% | 58% |

    GPU Usage after deleting the Tensors
    | ID | GPU | MEM |
    ------------------
    |  0 | 42% | 58% |

    GPU Usage after emptying the cache
    | ID | GPU | MEM |
    ------------------
    |  0 | 42% | 11% |
```

## ë°˜ë³µë¬¸ ë‚´ì— ì¶•ì ë˜ëŠ” tensor ë³€ìˆ˜
`torch.tensor`ë¡œ ì²˜ë¦¬ëœ ë³€ìˆ˜ëŠ” GPUë‚´ì— ìˆëŠ” ë©”ëª¨ë¦¬ ê³µê°„ì„ ì‚¬ìš©í•œë‹¤. ì´ ë³€ìˆ˜ëŠ” ë°˜ë³µë¬¸ë‚´ì— ì—°ì‚°ì´ ìˆëŠ” ê²½ìš°, GPUì— ê³„ì‚° ê·¸ë˜í”„ë¥¼ ìƒì„±í•´ì„œ ë©”ëª¨ë¦¬ë¥¼ ì ì‹í•˜ëŠ” í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤.

```py
# ì˜ˆì‹œ
total_loss = 0
for _ in range(10000):
    optimizer.zero_grad()
    predict = model(train_x)
    cost = criterion(predict)
    cost.backward()
    optimizer.step()
    total_loss += loss
```

### `.item()`ì˜ ì‚¬ìš©
ìœ„ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, 1ì°¨ì›ì˜ í…ì„œì¸ê²½ìš° `.item()`ì„ ì‚¬ìš©í•´ì„œ pythonì˜ ê¸°ë³¸ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.
```py
total_loss = 0
for _ in range(10000):
    loss = torch.randn(3, 4).mean()
    loss.requires_grad = True
    total_loss += loss.item() # total_loss += float(loss)
```

### `del`ì˜ ì‚¬ìš©
ë°˜ë³µë¬¸ ë‚´ì—ì„œë§Œ ì‹¤í–‰ë˜ê³  ë°˜ë³µë¬¸ ë°–ì—ì„œëŠ” í•„ìš”ì—†ëŠ” ë³€ìˆ˜ëŠ” ì ì ˆí•œ ì‚­ì œê°€ í•„ìš”í•˜ë‹¤. Pythonì˜ ë©”ëª¨ë¦¬ ë°°ì¹˜ íŠ¹ì„±ìƒ ë°˜ë³µë¬¸ì´ ì¢…ë£Œí•´ë„ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ê¸° ë•Œë¬¸ì— `del`í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì„œ ë©”ëª¨ë¦¬ ê´€ê³„ë¥¼ ëŠì–´ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤.

```py
for x in range(10):
    i = x
print(i) # 9 -> ë°˜ë³µë¬¸ ì™¸ì—ì„œë„ ë³€ìˆ˜ ê°’ì´ ì¶œë ¥ëœë‹¤.
```
```py
# delì„ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ì˜ íš¨ìœ¨ì  ì‚¬ìš©
for i in range(10):
    intermediate = f(input[i])
    result += g(intermediate)

del intermediate
output = h(result)

return output
```

## batch í¬ê¸° ì¤„ì´ê¸°
ë„ˆë¬´ í° í¬ê¸°ì˜ ë°°ì¹˜ì‚¬ì´ì¦ˆê°€ ë°°ì •ë˜ì–´ì„œ OOMì´ ë°œìƒë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ìµœì†Œ 1ê¹Œì§€ ì‹œí—˜ì„ í•´ë³¸ë‹¤.
```py
oom = False
try:
    run_model(batch_size)
except RuntimeError:
    oom = True

if oom:
    for _ in range(batch_size)
        run_modle(1)
```

## `torch.no_grad()`
ì¶”ë¡ í•˜ëŠ” ë‹¨ê³„ì—ì„œëŠ” gradientì˜ ê³„ì‚°ì´ í•„ìš”í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ì´ì „ì˜ í¬ìŠ¤íŒ…ì—ì„œ ë§í–ˆë˜ ê²ƒê³¼ ê°™ì´ `torch.no_grad()`ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì—­ì „íŒŒì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë©”ëª¨ë¦¬ì˜ ë²„í¼ì—ì„œ ë¶€í„° ììœ ë¡œì›Œì§€ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

```py
with torch.no_grad():
    for data, target in test_loader:
        output = network(data)
        test_loss += F.nll_loss(output, target, size_average=False).item()
        predict = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).sum()
```

## etc.
- Google Colab í™˜ê²½ì—ì„œëŠ” ë„ˆë¬´ í° ì‚¬ì´ì¦ˆì˜ ëª¨ë¸ì„ ì‹¤í–‰í•˜ì§€ ì•ŠëŠ”ê²ƒì´ ì¢‹ë‹¤. ex. linear, CNN, LSTM
- CNNì˜ ëŒ€ë¶€ë¶„ì˜ ì—ëŸ¬ëŠ” input, outputì˜ í¬ê¸°ê°€ ë§ì§€ ì•Šì•„ì„œ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤. ë”°ë¼ì„œ `torchsummary`ë¡œ ì‚¬ì´ì¦ˆë¥¼ ë§ì¶”ì–´ì¤˜ì•¼ í•œë‹¤.
- í…ì„œì˜ float precisionê°’ì„ 16ë¹„íŠ¸ë¡œ ì¤„ì—¬ì„œ ì‚¬ìš©í•œë‹¤. (í•˜ë‚˜ì˜ ê°’ì´ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì¤€ë‹¤.)

<br>

# References

[ğŸ“˜ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ì—†ëŠ” GPU ì—ëŸ¬ ì •ë¦¬(device-side assert, CUDA error, CUDNN_STATUS_NOT_INITIALIZED ë“±ë“±â€¦)](https://brstar96.github.io/shoveling/device_error_summary/)