---
title:  "[PyTorch] PyTorch Basics"
excerpt: "PyTorchì—ì„œ ë°ì´í„°ë“¤ì˜ ì—°ì‚°ì„ ìœ„í•œ Tensorì‚¬ìš©ë²•, ìë™ë¯¸ë¶„ ê¸°ëŠ¥ì¸ AutoGrad"

categories:
  - pytorch
tags:
  - [AI, Naver, BoostCamp, Python, Pytorch]
toc: true
toc_sticky: true
 
date: 2022-01-24 01:00:00
last_modified_at: 2022-01-24 01:00:00
---
ğŸ“Œ **ì•Œë¦½ë‹ˆë‹¤!**<br>
ì´ë²ˆì— ì‘ì„±ë˜ëŠ” ê¸€ì€ **ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech**ë¥¼ ìˆ˜ê°•í•˜ë©° ì •ë¦¬í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤.<br>
ì—¬ê¸°ì„œ ì¡´ì¬í•˜ëŠ” ê°•ì˜ ìë£Œì˜ ì¶œì²˜ëŠ” ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤/ìº í”„ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
{: .notice--info}

# PyTorch íŒ¨í‚¤ì§€
- torch : Tensorì™€ Numpyì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ ìˆ˜í•™ í•¨ìˆ˜ë“¤ì´ í¬í•¨ëœ ë©”ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
- torch.autograd : ìë™ë¯¸ë¶„(auto grad)ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ì´ í¬í•¨ë˜ì–´ ìˆë‹¤.
- torch.nn : ì‹ ê²½ë§ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°, ë ˆì´ì–´ê°€ ì •ì˜ë˜ì–´ ìˆë‹¤. (RNN, LSTM, ReLU, MSELoss)
- torch.optim : PyTorch Datasetì˜ Parameterì„ ìµœì í™” í•´ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ êµ¬í˜„ë˜ì–´ ìˆë‹¤. (SGD)
- torch.utils.data : SGDì˜ ë°˜ë³µì—°ì‚°ì„ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¯¸ë‹ˆ ë°°ì¹˜ìš© í•¨ìˆ˜ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.
- torch.onnx : Tensorflow, Kerasì™€ ê°™ì€ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì™€ PyTorch ë¥¼ ì—°ê²°ì‹œí‚¬ ë•Œ ì‚¬ìš©í•œë‹¤.

# Tensor
`í…ì„œ(Tensor)`ëŠ” ë‹¤ì°¨ì› Arrayë¥¼ í‘œí˜„í•˜ëŠ” PyTorchì˜ í´ë˜ìŠ¤ë¡œì„œ numpyì˜ ndarrayì™€ ë™ì¼í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ `TensorFlow`ì˜ tensorì™€ë„ ê·¸ ê°œë…ì´ ê°™ë‹¤.

ë‹¤ìŒì€ numpyì—ì„œ ndarrayë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œì™€ pytorchì—ì„œ tensorë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œì´ë‹¤. ë‘ ê°œë…ì´ ë™ì¼í•˜ê¸° ë•Œë¬¸ì— ì½”ë“œì—­ì‹œ ë¹„ìŠ·í•˜ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.

## ndarray(Numpy), tensor(PyTorch)
```py
import numpy as np

n_array = np.arange(10).reshape(2, 5)
print(n_array)
print("ndim :", n_array.ndim, "shape :", n_array.shape)
```
```py
import torch

t_array = torch.FloatTensor(n_array)
print(t_array)
print("ndim :", t_array.ndim, "shape :", t_array.shape)
```

## Array to Tensor
```py
data = [[3, 5], [10, 5]]
x_data = torch.tensor(data) # listí˜•íƒœì˜ ë°ì´í„°ë¥¼ torchë¥¼ ì‚¬ìš©í•´ tensorë¡œ ë³€í™˜
x_data
>>> tensor([[ 3,  5],
            [10,  5]])

nd_array_ex = np.array(data)
tensor_array = torch.from_numpy(nd_array_ex)
tensor_array
>>> tensor([[ 3,  5],
            [10,  5]], dtype=torch.int32)
```

## Tensorì˜ ìë£Œí˜•
í…ì„œê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìë£Œí˜•ì€ ê¸°ë³¸ì ìœ¼ë¡œ numpyì—ì„œ ì‚¬ìš©í•˜ëŠ” ìë£Œí˜•ê³¼ ë™ì¼í•˜ë‹¤. ì¶”ê°€ëœ ìë£Œí˜•ì´ ìˆë‹¤ë©´, GPUë¥¼ ì‚¬ìš©í•œ ì—°ì‚°ì„ ì§€ì›í•˜ëŠ” ìë£Œí˜•ì´ë‹¤.

`torch.cuda.FloatTensor`, `torch.cuda.DoubleTensor` ë“±.

ì¶”ê°€ë¡œ, ê¸°ë³¸ì ì¸ ë¬¸ë²• ì—­ì‹œ numpyì˜ ë¬¸ë²•ì´ PyTorchì—ì„œë„ ê·¸ëŒ€ë¡œ ì ìš©ì´ ëœë‹¤. numpyì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•œ **slicing**, numpyì˜ ë©”ì†Œë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

```py
data = [[3, 5, 20], [10, 5, 50], [1, 5, 10]]
x_data = torch.tensor(data)

x_data[1:]
>>> tensor([[10,  5, 50],
            [ 1,  5, 10]])

x_data[:2, 1:]
>>> tensor([[ 5, 20],
            [ 5, 50]])

x_data.flatten()
>>> tensor([ 3,  5, 20, 10,  5, 50,  1,  5, 10])

torch.ones_like(x_data) # ê½¤ ìì£¼ ì‚¬ìš©í•œë‹¤!
>>> tensor([[1, 1, 1],
            [1, 1, 1],
            [1, 1, 1]])

x_data.numpy()
>>> array([[ 3,  5, 20],
           [10,  5, 50],
           [ 1,  5, 10]], dtype=int64)

x_data.shape
>>> torch.Size([3, 3])

x_data.dtype
>>> torch.int64
```

<br>

# Tensor handling

## view

numpyì˜ `reshape()`ì™€ ë™ì¼í•˜ê²Œ tensorì˜ ëª¨ì–‘ì„ ë³€í™”ì‹œí‚¤ëŠ” í•¨ìˆ˜ì´ë‹¤. í•˜ì§€ë§Œ ë‘˜ ì‚¬ì´ì—ëŠ” ì•½ê°„ì˜ ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤.

[ğŸ” What's the difference between reshape and view in pytorch? - stackoverflow](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)

- `reshape()`: ê°€ëŠ¥í•˜ë©´ inputì˜ viewë¥¼ ë°˜í™˜í•˜ë©°, ê·¸ë ‡ì§€ ëª»í•œ ê²½ìš°ì—ëŠ” **contiguous**í•œ í…ì„œë¡œ ë³µí•˜ì‚¬ì—¬ viewë¥¼ ë°˜í™˜í•œë‹¤.
- `view()`: ê¸°ì¡´ì˜ ë°ì´í„°ì™€ ê°™ì€ ë©”ëª¨ë¦¬ ê³µê°„ì„ ê³µìœ í•˜ë©°, **contiguous**í•´ì•¼ë§Œ í•¨ìˆ˜ê°€ ì‹¤í–‰ëœë‹¤.

í•œ ë²ˆ ì˜ˆì‹œë¥¼ í†µí•´ì„œ ì•Œì•„ë³´ì.

1. í•¨ìˆ˜ëª…ì—ì„œ ì•Œ ìˆ˜ ìˆë‹¤ì‹œí”¼, `torch.veiw`ëŠ” ì›ë˜ì˜ í…ì„œì˜ viewë¥¼ ìƒì„±í•œë‹¤. ì´ë ‡ê²Œ ìƒì„±ëœ í…ì„œëŠ” í•­ìƒ ì›ë³¸ í…ì„œì™€ ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì›ë˜ì˜ í…ì„œ ê°’ì´ ë³€ê²½ë˜ë©´ `torch.veiw`ë¡œ ìƒì„±ëœ í…ì„œì˜ ê°’ë„ ë³€ê²½ì´ ì´ë£¨ì–´ì§„ë‹¤.

    ```py
    z = torch.zeros(3, 2) # ì›ë³¸ í…ì„œ zë¥¼ ìƒì„± (0ìœ¼ë¡œ ì±„ì›Œì§)
    x = z.view(2, 3) # ì›ë³¸ í…ì„œ zì— ëŒ€í•œ viewë¥¼ ìƒì„±
    z.fill_(1) # ì›ë³¸ í…ì„œì˜ ê°’ ë³€ê²½ì´ ì¼ì–´ë‚¨
    x # viewì—ë„ ê°’ì˜ ë³€ê²½ì´ ì¼ì–´ë‚¨ì„ í™•ì¸
    >>> tensor([[1., 1., 1.],
                [1., 1., 1.]])
    ```

2. `torch.view`ë¡œ ìƒˆë¡­ê²Œ ìƒì„œëœ í…ì„œê°€ ì›ë³¸ í…ì„œì™€ ê°’ì„ ê³µìœ í•˜ëŠ” ê²ƒì„ ë³´ì¥í•˜ê¸° ìœ„í•´, `torch.view`ëŠ” ìƒì„±ëœ 2ê°œì˜ í…ì„œê°„ì— **ì—°ì†ì„±ì— ëŒ€í•œ ì œì•½**ì„ ë‘”ë‹¤. ì¶”ê°€ë¡œ, ë‘ í…ì„œì˜ ëª¨ì–‘ì´ í˜¸í™˜ë˜ëŠ” ê²½ìš°ì—ë„ `torch.view`ì—ì„œëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

    ```py
    z = torch.zeros(3, 2)
    y = z.t() # í…ì„œ zì´ transpose
    y.size()
    >>> torch.Size([2, 3])

    y.view(6)
    >>> Traceback (most recent call last):
        File "<stdin>", line 1, in <module>
        RuntimeError: invalid argument 2: view size is not compatible with input tensor's
        size and stride (at least one dimension spans across two contiguous subspaces).
        Call .contiguous() before .view().
    ```

3. ë°˜ë©´ì— `torch.reshape`ëŠ” ì—°ì†ì„±ì— ëŒ€í•œ ì œì•½ì„ ë¶€ê³¼í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‹¹ì—°íˆ ì›ë³¸ í…ì„œì™€ì˜ ê°’ì„ ê³µìœ í•œë‹¤ëŠ” ë³´ì¥ë„ ì—†ê²Œ ëœë‹¤. `torch.reshape`ë¥¼ í•˜ëŠ” ê²½ìš°, ì›ë³¸ í…ì„œì˜ viewí˜•íƒœê°€ ë  ìˆ˜ë„ ìˆê³ , ì „í˜€ ê°’ì„ ê³µìœ í•˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ í…ì„œë¡œ ìƒì„±ë  ìˆ˜ ìˆë‹¤.

    ```py
    z = torch.zeros(3, 2)
    y = z.reshape(6)
    x = z.t().reshape(6)

    z.fill_(1)
    >>> tensor([[1., 1.],
                [1., 1.],
                [1., 1.]])

    y
    >>> tensor([1., 1., 1., 1., 1., 1.]) # reshape()í–ˆìŒì—ë„ ê°’ì„ ê³µìœ : viewí˜•íƒœ

    x
    >>> tensor([0., 0., 0., 0., 0., 0.]) # reshape()ë¥¼ í•˜ì—¬ ê°’ì´ ê³µìœ ë˜ì§€ ì•ŠëŠ” í˜•íƒœ
  ```

## squeeze, unsqueeze

![image](https://i.stack.imgur.com/9AJJA.png)

[ğŸ” Pytorch squeeze and unsqueeze - stackoverflow](https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze)

- `torch.squeeze`: ì°¨ì›ì˜ ê°œìˆ˜ê°€ 1ì¸ ì°¨ì›ì„ ì‚­ì œí•œë‹¤.
- `torch.unsqueeze`: ì°¨ì›ì˜ ê°œìˆ˜ê°€ 1ì¸ ì°¨ì›ì„ ì¶”ê°€í•œë‹¤.

ë‹¤ìŒ í…ì„œë¥¼ ë³´ë©´, 2ë²ˆì§¸ ì°¨ì›ì˜ ê°œìˆ˜ê°€ 1ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë•Œ, `torch.squeeze`ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ ì°¨ì›ì„ ì‚­ì œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

```py
tensor_ex = torch.rand(size=(2, 1, 2))
tensor_ex
>>> tensor([[[0.8369, 0.3459]],
            [[0.5851, 0.1484]]])

print(tensor_ex.squeeze())
>>> tensor([[0.8369, 0.3459],
            [0.5851, 0.1484]])

print(tensor_ex.squeeze().shape)
>>> torch.Size([2, 2])
```

```py
tensor_ex = torch.rand(size=(2, 2))

tensor_ex.unsqueeze(0).shape
>>> torch.Size([1, 2, 2])

tensor_ex.unsqueeze(1).shape
>>> torch.Size([2, 1, 2])

tensor_ex.unsqueeze(2).shape
torch.Size([2, 2, 1])
```

<br>

# Tensor Operations
í…ì„œì˜ ì—°ì‚°ì€ numpyì—ì„œ ì œê³µí•˜ëŠ” ì—°ì‚°ê³¼ ë™ì¼í•˜ê²Œ ìˆ˜í–‰ëœë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ í…ì„œê°„ì˜ ì—°ì‚°ì—ì„œëŠ” ë™ì¼í•œ í¬ê¸°ë¥¼ ê°€ì§„ê²ƒ ë¼ë¦¬ ì—°ì‚°ì´ ëœë‹¤.

## í…ì„œê°„ì˜ ì‚¬ì¹™ì—°ì‚°
```py
n1 = np.arange(10).reshape(2, 5)
n2 = np.arange(10).reshape(5, 2)
t1 = torch.FloatTensor(n1)
t2 = torch.FloatTensor(n2)

t1 + t1
t1 - t1
t1 + 10
```

## í…ì„œê°„ì˜ ê³±ì…ˆì—°ì‚°
í•˜ì§€ë§Œ, í–‰ë ¬ì˜ ê³±ì…ˆ ì—°ì‚°ì„ ìˆ˜í–‰í•  ë•ŒëŠ” `pytorch.dot`ì´ ì•„ë‹Œ `pytorch.mm`ì´ë‚˜ `pytorch.matmul`ì„ ì‚¬ìš©í•œë‹¤.

```py
t1.mm(t2) # í–‰ë ¬ì˜ ê³±ì…ˆ ì‹¤í–‰
>>> tensor([[ 60.,  70.],
            [160., 195.]])

t1.dot(t2)
>>> RuntimeError: 1D tensors expected, but got 2D and 2D tensors

t1.matmul(t2)
>>> tensor([[ 60.,  70.],
            [160., 195.]])
```

### mmê³¼ matmulì˜ ì°¨ì´

`pytorch.mm`ê³¼ `pytorch.matmul`ì˜ ì°¨ì´ëŠ” ì—°ì‚°ì—ì„œ **boradcasting**ì„ ì§€ì›í•˜ëŠ”ì§€ì˜ ì—¬ë¶€ì—ì„œ ë°œìƒí•œë‹¤. ë¨¼ì € `pytorch.mm`ì€ broadcastingì„ ì§€ì›í•˜ì§€ ì•Šê³ , `pytorch.matmul`ëŠ” broadcastingì„ ì§€ì›í•œë‹¤.

```py
a = torch.rand(5, 2, 3)
b = torch.rand(3)

a.mm(b)
>>> NameError: name 'torch' is not defined

a.matmul(b)
tensor([[0.6014, 0.8045],
        [0.9585, 1.1915],
        [0.9335, 0.8395],
        [1.3930, 0.8872],
        [1.1288, 1.4620]])
```

> `matmul`ì˜ ì—°ì‚°ì€ `mm`ì´ í–‰ë ¬ aì˜ í–‰ì— ëŒ€í•´ì„œ ìˆ˜í–‰ëœë‹¤ê³  ìƒê°í•´ë„ ëœë‹¤.

```py
a[0].mm(torch.unsqueeze(b, 1))
a[1].mm(torch.unsqueeze(b, 1))
a[2].mm(torch.unsqueeze(b, 1))
a[3].mm(torch.unsqueeze(b, 1))
a[4].mm(torch.unsqueeze(b, 1))
```

## ML, DLì—ì„œ ì‚¬ìš©í•˜ëŠ” ì—°ì‚°
PyTorchëŠ” `torch.nn.functional`ì´ë¼ëŠ” ëª¨ë“ˆì„ í†µí•´ì„œ ë‹¤ì–‘í•œ ìˆ˜ì‹ë³€í™˜ì„ ì§€ì›í•œë‹¤. ê¸°ë³¸ì ì¸ í™œì„±í•¨ìˆ˜ë¥¼ ì§€ì›í•˜ê³  ê·¸ ì™¸ì—ë„ ì›í•«ë²¡í„°ë¡œì˜ ë³€í™˜ë“± ì—¬ëŸ¬ê°€ì§€ ì—°ì‚°ë“¤ì´ í¬í•¨ë˜ì–´ ìˆë‹¤.

```py
import torch.nn.functional as F

tensor = torch.FloatTensor([0.5, 0.7, 0.1])
h_tensor = F.softmax(tensor, dim = 0)
h_tensor
>>> tensor([0.3458, 0.4224, 0.2318])

y = torch.randint(5, (10, 5))
y_label = y.argmax(dim = 1)
F.one_hot(y_label)

>>> tensor([[0, 0, 1, 0],
            [0, 0, 1, 0],
            [0, 1, 0, 0],
            [1, 0, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
            [0, 1, 0, 0],
            [0, 1, 0, 0],
            [0, 0, 0, 1],
            [0, 0, 1, 0]])
```

ì¶”ê°€ë¡œ, ê°€ëŠ¥í•œ ëª¨ë“  ìŒì„ ê³ ë ¤í•´ì£¼ëŠ” cartesian_productë¥¼ ì§€ì›í•œë‹¤.

```py
import itertools
a = [1, 2, 3]
b = [4, 5]
list(itertools.product(a, b))
>>> [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]

tensor_a = torch.tensor(a)
tensor_b = torch.tensor(b)
torch.cartesian_prod(tensor_a, tensor_b)
>>> tensor([[1, 4],
            [1, 5],
            [2, 4],
            [2, 5],
            [3, 4],
            [3, 5]])
```

## autograd
PyTorchì˜ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” **ìë™ë¯¸ë¶„**ì—°ì‚°ì„ ì§€ì›í•œë‹¤. ì´ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” `pytorch.backward()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ì„œ ë‹¤ìŒ í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤ê³  í•  ë•Œ, \\(z\\)ë¥¼ \\(w\\)ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.

$$ y = w^2 $$

$$ z = 10y^2 + 25 $$

$$ \frac{\partial z}{\partial w} = 20w $$

```py
w = torch.tensor(2.0, requires_grad  = True)
y = w**2
z = 10*y + 25
z.backward()
w.grad
>>> tensor(40.)
```

ë˜ ë‹¤ë¥¸ ì˜ˆì œë¡œ ë‹¤ìŒì„ í•œ ë²ˆ ë³´ì. ë¯¸ë¶„ì„ ì§„í–‰í•˜ê³  ê±°ê¸°ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ëŒ€ì…í•  ë•Œ, listí˜•íƒœë¡œ ì „ë‹¬ì„ í•´ì„œ ì—¬ëŸ¬ ê°’ì— ëŒ€í•œ ë¯¸ë¶„ê°’ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

$$ Q = 3a^3 - b^2 $$

$$ \frac{\partial Q}{\partial a} = 9a^2 $$

$$ \frac{\partial Q}{\partial b} = -2b $$

```py
a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)

Q = 3*a**3 - b**2
external_grad = torch.tensor([1., 1.]) # ë¯¸ë¶„ì„ í•´ì„œ ì•Œì•„ë‚´ê³  ì‹¶ì€ ê°’ì˜ ìˆ˜ ë§Œí¼. 
Q.backward(gradient = external_grad)

a.grad
>>> tensor([36., 81.])

b.grad
>>> tensor([-12.,  -8.])
```

ìœ„ì˜ ì½”ë“œì—ì„œ `external_grad`ëŠ” ì—­ì „íŒŒë¥¼ ì§„í–‰í•˜ë©´ì„œ ì•Œì•„ë‚´ê³  ì‹¶ì€ ê°’ì˜ ê°œìˆ˜ë¥¼ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ ì§„í–‰í•œë‹¤. ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¨ ë¯¸ë¶„ê°’ì— external_gradê°’ì„ ê³±í•´ì ¸ì„œ ê²°ê³¼ë¡œ ì¶œë ¥ë˜ëŠ”ë° ì˜í–¥ì„ ì£¼ê³  ì‹¶ì§€ ì•Šì„ ê²½ìš° 1ë¡œ ì„¤ì •í•œë‹¤.

<br>

# References

[ğŸ“˜ [PyTorch] view, reshape, transpose, permuteí•¨ìˆ˜ì˜ ì°¨ì´ì™€ contiguous](https://sanghyu.tistory.com/3)

[ğŸ“˜ What's the difference between reshape and view in pytorch?](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)

[ğŸ“˜ Pytorch squeeze and unsqueeze - stackoverflow](https://stackoverflow.com/questions/61598771/pytorch-squeeze-and-unsqueeze)

[ğŸ“˜ [DL, PyTorch] ì°¨ì›ì¬êµ¬ì„± - tistory](https://anweh.tistory.com/12)