---
title:  "[부스트캠프 Pre-Course] 딥러닝 기초: 뉴럴 네트워크-MLP"
excerpt: "신경망의 정의와 Deep Neural Networks"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Python, Math]
toc: true
toc_sticky: true
 
date: 2021-12-08
last_modified_at: 2021-12-08
---

# <span style = "color: #00adb5">Neural Networks</span>
> Neural Networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains. - Wikipedia

> Neural networks are function approximator that stack affine transformations followed by nonlinear transformations.

<br>

# <span style = "color: #00adb5">Linear Neural Networks</span>

![image](https://user-images.githubusercontent.com/91870042/145202345-9b5040d0-4de3-4961-a515-0b98fb06f0ed.png){: .align-center}

위의 그림에서 *Data, Model, Loss* 는 아래와 같이 수식으로 나타낼 수 있다.
- Data: \\(D=\{(x_i, y_i)\}^N_{i=1}\\)
- Model: \\(\hat{y}=wx+b\\)
- Loss: \\(loss=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y_i})^2\\)

찾고자 하는값은 \\(w, b\\)인데 이 값은  `loss function`을 \\(w, b\\)에 대해서 각각 미분하여 나온 결과값을 이용한다. 이전에 배웠던 `경사하강법`을 이용해 값을 업데이트해 나가면서 가장 잘 예측하는 값을 찾는 것이다.

> 먼저, *loss function* 을 \\(w\\)에 대해 미분한 값에 대해서 보자.

![image](https://user-images.githubusercontent.com/91870042/145204518-a68f89fc-6936-48b4-876c-05d46f137480.png){: .align-center}

> 그 다음, *loss function* 을 \\(b\\)에 대해 미분한 값에 대해서 보자.

![image](https://user-images.githubusercontent.com/91870042/145204699-7d415d83-1450-44bc-8d57-9f461d1fa0fe.png){: .align-center}

위에서 나온 식을 이용해서 \\(w, b\\)의 값을 이용해 특정 step_size(\\(\eta\\)) 만큼을 곱한다음에 빼주는 방식으로 업데이트를 진행한다. \\(\eta\\) 의 설정도 중요한데 이 값이 너무 커지면 학습이 되지 않는다. 또 반대로 너무 적으면 학습이 이루어지지 않는다. `Adaptive Learning Rate`를 사용하면 해당 \\(\eta\\) 값을 자동으로 바꿔가면서 학습을 진행한다.

![image](https://user-images.githubusercontent.com/91870042/145204833-cd80435d-a72d-4be7-9844-12424d172165.png){: .align-center}

## Multi Dimension Neural Networks

하지만, 세상이 방금과 같은 예시처럼 선형으로만 이루어져 있거나, 1차원 입력에서 1차원 출력으로 나오는 경우는 많지 않다. 그럴 때는 행렬을 사용해야 한다.

![image](https://user-images.githubusercontent.com/91870042/145207505-0478b9ca-a1d5-49f8-98b9-f6b87d537219.png){: .align-center}

앞에서의 \\(w, b\\)는 값이었던 반면, 이번에는 \\(W\\)는 행렬을, \\(b\\)는 벡터를 의미한다.
\\(
    y = W^Tx + b
\\)
이렇게 곱해지는 행렬(\\(W\\))을 해석하는 하나의 방법은, 두개의 벡터사이의 변환이다.

## Beyond Linear Neural Networks

![image](https://user-images.githubusercontent.com/91870042/145208883-2f6ecfa6-6e05-46e1-8948-73eb26548c90.png){: .align-center}

위 사진 처럼, 딥러닝으로 Neural Network를 여러겹 쌓겠다고 해보자. Neural NEtwork를 여러개 쌓기 위해서는 하나의 Network가 가중치 행렬 \\(W_{1}^{T}x)\\)을 통해 나오는 결과 Hidden Vector(\\(h\\))를 다시 가중치 행렬에 넣어 결과를 얻어내는 형식으로 사용한다. 이를 수식으로 표현하면 다음과 같다.

\\(
    y = W_{2}^{T}h = W_{2}^{T}W_{1}^{T}x
\\)

사실, 위의 수식은 처음의 입력에 대해서 가중치 행렬을 2번 연산한 결과와 동일한데, 이는 1층의 Network Layer와 동일하다. 그렇기 때문에 이 값을 증폭시켜줄 `비선형함수`가 필요한데 우리는 그 함수를 `활성함수(Activation Function)`이라고 부른다. 따라서 활성함수를 \\(\rho\\) 라고 하면 다시 다음과 같아 나타낼 수 있다.

\\(
    y = W_{2}^{T}h = W_{2}^{T}\rho(W_{1}^{T}x)
\\)

## 활성 함수
위에서 말한 활성함수의 종류에는 `ReLU`, `Sigmoid`, `Hyperbolic Tangent`함수가 있다. 어떤게 좋은지는 문제와 상황마다 모두 다르다. 하지만, Neural Network를 여러개 쌓았을 때, 의미있는 결과를 얻어내기 위해서는 사용해야하는 비선형 함수이다.

![image](https://user-images.githubusercontent.com/91870042/145209921-f2d2a57f-1ece-4b2f-b793-3b2f057be4cf.png){: .align-center}

## Multi-layer Perceptron
지금까지 알아본 이론을 이용해서 Nerual Network를 2층이 아닌 훨씬 더 깊게 설정할 수 있다. 예를 들어, 3층으로 Neural Network를 구성한다고 하면 다음과 같은 수식이 나온다.

![image](https://user-images.githubusercontent.com/91870042/145210135-0df9dd58-6721-4c62-a9b9-cba0c29a3c6c.png){: .align-center}

## 손실 함수
loss function을 선택을 해야하는데, 흔히 다음 3가지 문제에 대해서는 각각 `MSE`, `CE`, `MLE` 손실함수를 사용한다. 실제로 손실 함수를 선택할 때, 이 함수가 왜 잘 평가할 수 있는지도 같이 설명을 해야한다.

![image](https://user-images.githubusercontent.com/91870042/145210298-3ae02cf8-6994-4d4f-a9d3-61972a9b1a3c.png){: .align-center}

<br>

# <span style = "color: #00adb5">References</span>
[📘부스트캠프 AI Tech 3기 Pre-Course: 뉴럴 네트워크-MLP](https://www.boostcourse.org/onlyboostcampaitech3/lecture/1203306/?isDesc=false)