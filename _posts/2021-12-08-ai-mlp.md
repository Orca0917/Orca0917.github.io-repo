---
title:  "[ë¶€ìŠ¤íŠ¸ìº í”„ Pre-Course] ë”¥ëŸ¬ë‹ ê¸°ì´ˆ: ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬-MLP"
excerpt: "ì‹ ê²½ë§ì˜ ì •ì˜ì™€ Deep Neural Networks"

categories:
  - boostcamp
tags:
  - [AI, Naver, BoostCamp, Python, Math]
toc: true
toc_sticky: true
 
date: 2021-12-08
last_modified_at: 2021-12-08
---

# <span style = "color: #00adb5">Neural Networks</span>
> Neural Networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains. - Wikipedia

> Neural networks are function approximator that stack affine transformations followed by nonlinear transformations.

<br>

# <span style = "color: #00adb5">Linear Neural Networks</span>

![image](https://user-images.githubusercontent.com/91870042/145202345-9b5040d0-4de3-4961-a515-0b98fb06f0ed.png){: .align-center}

ìœ„ì˜ ê·¸ë¦¼ì—ì„œ *Data, Model, Loss* ëŠ” ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
- Data: \\(D=\{(x_i, y_i)\}^N_{i=1}\\)
- Model: \\(\hat{y}=wx+b\\)
- Loss: \\(loss=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y_i})^2\\)

ì°¾ê³ ì í•˜ëŠ”ê°’ì€ \\(w, b\\)ì¸ë° ì´ ê°’ì€  `loss function`ì„ \\(w, b\\)ì— ëŒ€í•´ì„œ ê°ê° ë¯¸ë¶„í•˜ì—¬ ë‚˜ì˜¨ ê²°ê³¼ê°’ì„ ì´ìš©í•œë‹¤. ì´ì „ì— ë°°ì› ë˜ `ê²½ì‚¬í•˜ê°•ë²•`ì„ ì´ìš©í•´ ê°’ì„ ì—…ë°ì´íŠ¸í•´ ë‚˜ê°€ë©´ì„œ ê°€ì¥ ì˜ ì˜ˆì¸¡í•˜ëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.

> ë¨¼ì €, *loss function* ì„ \\(w\\)ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì— ëŒ€í•´ì„œ ë³´ì.

![image](https://user-images.githubusercontent.com/91870042/145204518-a68f89fc-6936-48b4-876c-05d46f137480.png){: .align-center}

> ê·¸ ë‹¤ìŒ, *loss function* ì„ \\(b\\)ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì— ëŒ€í•´ì„œ ë³´ì.

![image](https://user-images.githubusercontent.com/91870042/145204699-7d415d83-1450-44bc-8d57-9f461d1fa0fe.png){: .align-center}

ìœ„ì—ì„œ ë‚˜ì˜¨ ì‹ì„ ì´ìš©í•´ì„œ \\(w, b\\)ì˜ ê°’ì„ ì´ìš©í•´ íŠ¹ì • step_size(\\(\eta\\)) ë§Œí¼ì„ ê³±í•œë‹¤ìŒì— ë¹¼ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•œë‹¤. \\(\eta\\) ì˜ ì„¤ì •ë„ ì¤‘ìš”í•œë° ì´ ê°’ì´ ë„ˆë¬´ ì»¤ì§€ë©´ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ë˜ ë°˜ëŒ€ë¡œ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤. `Adaptive Learning Rate`ë¥¼ ì‚¬ìš©í•˜ë©´ í•´ë‹¹ \\(\eta\\) ê°’ì„ ìë™ìœ¼ë¡œ ë°”ê¿”ê°€ë©´ì„œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145204833-cd80435d-a72d-4be7-9844-12424d172165.png){: .align-center}

## Multi Dimension Neural Networks

í•˜ì§€ë§Œ, ì„¸ìƒì´ ë°©ê¸ˆê³¼ ê°™ì€ ì˜ˆì‹œì²˜ëŸ¼ ì„ í˜•ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì ¸ ìˆê±°ë‚˜, 1ì°¨ì› ì…ë ¥ì—ì„œ 1ì°¨ì› ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ê²½ìš°ëŠ” ë§ì§€ ì•Šë‹¤. ê·¸ëŸ´ ë•ŒëŠ” í–‰ë ¬ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145207505-0478b9ca-a1d5-49f8-98b9-f6b87d537219.png){: .align-center}

ì•ì—ì„œì˜ \\(w, b\\)ëŠ” ê°’ì´ì—ˆë˜ ë°˜ë©´, ì´ë²ˆì—ëŠ” \\(W\\)ëŠ” í–‰ë ¬ì„, \\(b\\)ëŠ” ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤.
\\(
    y = W^Tx + b
\\)
ì´ë ‡ê²Œ ê³±í•´ì§€ëŠ” í–‰ë ¬(\\(W\\))ì„ í•´ì„í•˜ëŠ” í•˜ë‚˜ì˜ ë°©ë²•ì€, ë‘ê°œì˜ ë²¡í„°ì‚¬ì´ì˜ ë³€í™˜ì´ë‹¤.

## Beyond Linear Neural Networks

![image](https://user-images.githubusercontent.com/91870042/145208883-2f6ecfa6-6e05-46e1-8948-73eb26548c90.png){: .align-center}

ìœ„ ì‚¬ì§„ ì²˜ëŸ¼, ë”¥ëŸ¬ë‹ìœ¼ë¡œ Neural Networkë¥¼ ì—¬ëŸ¬ê²¹ ìŒ“ê² ë‹¤ê³  í•´ë³´ì. Neural NEtworkë¥¼ ì—¬ëŸ¬ê°œ ìŒ“ê¸° ìœ„í•´ì„œëŠ” í•˜ë‚˜ì˜ Networkê°€ ê°€ì¤‘ì¹˜ í–‰ë ¬ \\(W_{1}^{T}x)\\)ì„ í†µí•´ ë‚˜ì˜¤ëŠ” ê²°ê³¼ Hidden Vector(\\(h\\))ë¥¼ ë‹¤ì‹œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ë„£ì–´ ê²°ê³¼ë¥¼ ì–»ì–´ë‚´ëŠ” í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

\\(
    y = W_{2}^{T}h = W_{2}^{T}W_{1}^{T}x
\\)

ì‚¬ì‹¤, ìœ„ì˜ ìˆ˜ì‹ì€ ì²˜ìŒì˜ ì…ë ¥ì— ëŒ€í•´ì„œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ 2ë²ˆ ì—°ì‚°í•œ ê²°ê³¼ì™€ ë™ì¼í•œë°, ì´ëŠ” 1ì¸µì˜ Network Layerì™€ ë™ì¼í•˜ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ ê°’ì„ ì¦í­ì‹œì¼œì¤„ `ë¹„ì„ í˜•í•¨ìˆ˜`ê°€ í•„ìš”í•œë° ìš°ë¦¬ëŠ” ê·¸ í•¨ìˆ˜ë¥¼ `í™œì„±í•¨ìˆ˜(Activation Function)`ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë”°ë¼ì„œ í™œì„±í•¨ìˆ˜ë¥¼ \\(\rho\\) ë¼ê³  í•˜ë©´ ë‹¤ì‹œ ë‹¤ìŒê³¼ ê°™ì•„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

\\(
    y = W_{2}^{T}h = W_{2}^{T}\rho(W_{1}^{T}x)
\\)

## í™œì„± í•¨ìˆ˜
ìœ„ì—ì„œ ë§í•œ í™œì„±í•¨ìˆ˜ì˜ ì¢…ë¥˜ì—ëŠ” `ReLU`, `Sigmoid`, `Hyperbolic Tangent`í•¨ìˆ˜ê°€ ìˆë‹¤. ì–´ë–¤ê²Œ ì¢‹ì€ì§€ëŠ” ë¬¸ì œì™€ ìƒí™©ë§ˆë‹¤ ëª¨ë‘ ë‹¤ë¥´ë‹¤. í•˜ì§€ë§Œ, Neural Networkë¥¼ ì—¬ëŸ¬ê°œ ìŒ“ì•˜ì„ ë•Œ, ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì–´ë‚´ê¸° ìœ„í•´ì„œëŠ” ì‚¬ìš©í•´ì•¼í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì´ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145209921-f2d2a57f-1ece-4b2f-b793-3b2f057be4cf.png){: .align-center}

## Multi-layer Perceptron
ì§€ê¸ˆê¹Œì§€ ì•Œì•„ë³¸ ì´ë¡ ì„ ì´ìš©í•´ì„œ Nerual Networkë¥¼ 2ì¸µì´ ì•„ë‹Œ í›¨ì”¬ ë” ê¹Šê²Œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 3ì¸µìœ¼ë¡œ Neural Networkë¥¼ êµ¬ì„±í•œë‹¤ê³  í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì´ ë‚˜ì˜¨ë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145210135-0df9dd58-6721-4c62-a9b9-cba0c29a3c6c.png){: .align-center}

## ì†ì‹¤ í•¨ìˆ˜
loss functionì„ ì„ íƒì„ í•´ì•¼í•˜ëŠ”ë°, í”íˆ ë‹¤ìŒ 3ê°€ì§€ ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ê°ê° `MSE`, `CE`, `MLE` ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì‹¤ì œë¡œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„ íƒí•  ë•Œ, ì´ í•¨ìˆ˜ê°€ ì™œ ì˜ í‰ê°€í•  ìˆ˜ ìˆëŠ”ì§€ë„ ê°™ì´ ì„¤ëª…ì„ í•´ì•¼í•œë‹¤.

![image](https://user-images.githubusercontent.com/91870042/145210298-3ae02cf8-6994-4d4f-a9d3-61972a9b1a3c.png){: .align-center}

<br>

# <span style = "color: #00adb5">References</span>
[ğŸ“˜ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech 3ê¸° Pre-Course: ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬-MLP](https://www.boostcourse.org/onlyboostcampaitech3/lecture/1203306/?isDesc=false)