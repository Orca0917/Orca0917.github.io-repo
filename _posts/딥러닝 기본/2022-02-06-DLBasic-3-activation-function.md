---
title:  "활성함수의 역할과 종류"
excerpt: "활성함수가 딥러닝에서 어떤 역할을 수행하는지, 그리고 활성함수에는 어떤 종류들이 있는지에 대해서 소개합니다."

categories:
  - DLBasic
tags:
  - [AI, Naver, BoostCamp, Math]
toc: true
toc_sticky: true
 
date: 2022-02-07 02:00:00
last_modified_at: 2022-06-17 02:00:00
---
📌 **알립니다!**<br>
이번에 작성되는 글은 **네이버 부스트캠프 AI Tech**를 수강하며 정리하는 글입니다.<br>
여기서 존재하는 강의 자료의 출처는 네이버 부스트코스/캠프에게 있습니다.
{: .notice--info}

# Activation Function

지난 포스팅인 [MLP에 대한 이해](./2-MLP.md)에서 선형성을 띄는 단층 퍼셉트론에서 비선형성을 띄는 다층 퍼셉트론으로 넘어가는데 필요한 것이 활성함수라고 소개했습니다. 이번에는 이 활성함수가 실제로 어떤식으로 딥러닝 또는 신경망 구조에 기여를 하는지, 활성함수의 각 종류와 장단점 그리고 특징에 대해서 하나씩 살펴보겠습니다! 😊

## 💫 신경망에서 활성함수

단층 퍼셉트론에서는 출력으로 $\theta$ 값을 기준으로 하여 0 또는 1의 값을 반환했었습니다. 실제로 신경망 구조에서는 이와 같은 연산을 하기도 하지만 소수이며, 대부분 0과 1사이의 실수값을 다음 신경망의 입력으로 전달합니다.

활성함수는 선형 연산을 거친 결과값을 다시 한 번 함수를 지나 0과 1사이의 값으로 만들어주는 역할을 수행합니다. 어떻게 보면 값에서 값으로 나오는 것이기 때문에 의미 없다고 생각할 수 있지만, 여기서 활성함수는 값을 변환하기보다는 비선형성을 추가하는데 더 큰 기여를 하고 있습니다.

이 활성함수의 종류에는 대표적인 3가지로 **sigmoid, tanh, ReLU**가 존재합니다. 이 외에 이 3가지가 변형된 활성함수나 완전히 다른 함수도 많으나 이번에는 주요 활성함수 몇 가지만 다루도록 하겠습니다.

<br>

## 🔨 Step function

step function은 계단함수라고도 불리기도 합니다. 사실 지금까지 설명했던 threshold 기반으로 0과 1로 반환하는 함수가 바로 이 step function에 해당합니다. step function을 그래프로 그려보면 다음과 같습니다.

![image](https://user-images.githubusercontent.com/91870042/174258696-822f4010-5c78-4e31-9814-57b8ee3a76f2.png){: .align-center width="50%"}

임계값 0을 기준으로 작다면 0을, 크다면 1을 반환하는 함수로 간단한 구조를 띄고 있습니다. 하지만 이 함수는 0의 지점에서 미분이 불가능하기 때문에 실제로 잘 사용되지는 않습니다. 활성함수의 미분가능성이 중요한 이유는 이후 **오차역전파** 글에서 더 자세히 다루겠습니다.

step function의 또 다른 문제점은 너무 극단적으로 값을 다음 신경망에 전달한다는 점입니다. 2개의 값 0 또는 1의 값만 전달되기 때문에 정보가 손실될 수 있는 심각한 문제점이 존재합니다.

<br>

## ⛏️ Sigmoid
