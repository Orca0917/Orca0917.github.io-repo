---
title:  "활성함수의 역할과 종류"
excerpt: "활성함수가 딥러닝에서 어떤 역할을 수행하는지, 그리고 활성함수에는 어떤 종류들이 있는지에 대해서 소개합니다."

categories:
  - DLBasic
tags:
  - [AI, Naver, BoostCamp, Math]
toc: true
toc_sticky: true
 
date: 2022-02-07 02:00:00
last_modified_at: 2022-06-17 02:00:00
---
![image](https://user-images.githubusercontent.com/91870042/174324069-e4a9c258-1d76-43a3-8ea4-71d0a9725519.png)

# Activation Function

지난 포스팅인 [MLP에 대한 이해](https://killerwhale0917.github.io/dlbasic/DLBasic-2-MLP/)에서 선형성을 띄는 단층 퍼셉트론에서 비선형성을 띄는 다층 퍼셉트론으로 넘어가는 데 필요한 것이 활성함수라고 소개했습니다. 이번에는 이 활성함수가 실제로 어떤 식으로 딥러닝 또는 신경망 구조에 기여를 하는지, 활성함수의 각 종류와 장단점 그리고 특징에 대해서 하나씩 살펴보겠습니다! 😊

## 💫 신경망에서 활성함수

단층 퍼셉트론에서는 출력으로 $\theta$ 값을 기준으로 하여 0 또는 1의 값을 반환했었습니다. 실제로 신경망 구조에서는 이와 같은 연산을 하기도 하지만 드물며, 대부분 0과 1 사이의 실수값을 다음 신경망의 입력으로 전달합니다.

활성함수는 선형 연산을 거친 결과 값이 다시 한 번 함수를 지나 또 다른 값으로 만들어주는 구실을 합니다. 어떻게 보면 값에서 값으로 나오는 것이기 때문에 의미 없다고 생각할 수 있지만, 여기서 활성함수는 값을 변환하기보다는 비선형성을 추가하는데 더 크게 이바지하고 있습니다. 다르게 이해하면, 다음 뉴런으로 신호를 얼마나 강하게 전달하는지 재설정한다고 볼 수 있습니다.

이 활성함수의 종류에는 대표적인 3가지로 **sigmoid, tanh, ReLU**가 존재합니다. 이 외에 이 3가지가 변형된 활성함수나 완전히 다른 함수도 많으나 이번에는 주요 활성함수 몇 가지만 다루도록 하겠습니다.

<br>

## 🔨 Step function

step function은 계단함수라고 불리기도 합니다. 사실 지금까지 설명했던 threshold 기반으로 0과 1로 반환하는 함수가 바로 이 step function에 해당합니다. step function을 그래프로 그려보면 다음과 같습니다.

![temp](https://user-images.githubusercontent.com/91870042/174318376-654347fe-f492-43dd-beed-1a5feac092c1.png){: .align-center width="55%"}

$$
\begin{aligned}
h(x)=\left\{\begin{matrix}
0\,(x \le 0) \\
1\,(x > 0)
\end{matrix}\right.
\end{aligned}
$$

임계값 0을 기준으로 작다면 0을, 크다면 1을 반환하는 함수로 간단한 구조를 띠고 있습니다. 하지만 이 함수는 0의 지점에서 미분이 불가능하여서 실제로 잘 사용되지는 않습니다. 활성함수의 미분 가능성이 중요한 이유는 이후 **오차역전파** 글에서 더 자세히 다루겠습니다.

step function의 또 다른 문제점은 너무 극단적으로 값을 다음 신경망에 전달한다는 점입니다. 2개의 값, 0 또는 1의 값만 전달되기 때문에 정보가 손실될 수 있는 심각한 문제점이 존재합니다.

<br>

## ⛏️ Sigmoid

시그모이드 함수는 길게 늘인 S와 같은 형태를 띠는 활성함수입니다. 먼저 시그모이드의 생김새를 살펴보면 다음과 같습니다.

![temp](https://user-images.githubusercontent.com/91870042/174318715-2fd60c6d-0361-48e8-987f-facac05bea05.png){: .align-center width="55%"}

$$
h(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{e^x + 1}
$$

시그모이드 함수는 전 구간에서 미분할 수 있으며, 단조증가 하는 특징이 있습니다. 출력범위도 0 ~ 1 사이의 실수값을 가지며 기울기가 급격하게 변화하지 않기 때문에 학습할 때 ***기울기 폭주 문제**가 발생하지 않습니다.

이 시그모이드 함수를 미분한 형태를 알아봅시다. 시그모이드 함수를 미분하게 되면 최댓값이 0.25인 종모양의 그래프가 그려집니다.

![temp](https://user-images.githubusercontent.com/91870042/174321221-f8af78ab-9e74-4ea4-a51f-963a0605a41f.png){: .align-center width="55%"}

미분했을 때 그래프가 연속적이므로 학습을 하는 데 있어서 긍정적이라고 생각할 수 있지만, 기울기 폭주 대신 ***기울기 소실 문제**가 발생할 수 있습니다. 그래서 실제로 시그모이드를 사용되는 경우는 잘 없습니다.

🤔 **기울기 소실과 기울기 폭주**<br>
아직 오차역전파법에 대해서 다루지는 않았지만, 오차를 신경망의 역방향으로 한 층씩 전달하면서 미분된 값과 오차가 곱해지게 됩니다. 하지만 시그모이드와 같은 경우 값이 매우 크거나 작다면 0인 값이 나오고, 계속 곱해지게 되면 기울기가 0에 가까워지는 문제가 있습니다. 이 문제를 우리는 기울기 소실문제 (Vanishing Gradient  Problem, VGP)라고 합니다. 반대로 기울기가 계속 증가하는 문제를 (Exploding Gradient Problem)이라고 합니다.
{: .notice--success}

<br>

## 🪓 Hyperbolic Tangent

하이퍼볼릭 탄젠트함수는 `tanh` 라고 표현하며 우리말로는 쌍곡선 탄젠트함수라고 합니다. tanh는 앞서 설명한 sigmoid 활성함수의 단점을 어느 정도 보완할 수 있는 활성함수이며 이제부터 어떤 특징을 가졌는지 살펴보겠습니다.

tanh는 sigmoid와 상당히 유사한 모습을 띠는 활성함수입니다. sigmoid와 마찬가지로 단조 증가하며 S 형태의 그래프로 표현할 수 있습니다.

![temp](https://user-images.githubusercontent.com/91870042/174322074-007f6581-6876-4b89-bc65-3956194e7353.png){: .align-center width="55%"}

$$
h(x) = \frac{1-e^{-x}}{1+e^{-x}}
$$

시그모이드 함수와의 차이점은 그 출력값의 구간이 0 ~ 1이 아닌 -1 ~ 1이라는 점입니다. 출력값이 더 넓으므로 학습에 더 효과적일 수 있으며 미분한 값의 구간도 0 ~ 1의 값을 갖게 되어 VGP 문제에 그나마 안전할 수 있습니다.

![image](https://user-images.githubusercontent.com/91870042/174310125-6f72cc76-2e0e-4221-a9c1-6a71cb8585b2.png){: .align-center width="80%"}

하지만 여전히 기울기 소실문제가 발생할 수 있는 위험은 있다는 것이 큰 단점으로 지적되고 있습니다.

<br>

## 🔧 ReLU

현재 딥러닝에서도 꽤 자주 등장하는 활성함수인 ReLU입니다. ReLU는 Rectified Linear Unit의 약자로 지금까지 살펴본 활성함수와는 조금 독특한 형태를 띠고 있습니다. Step Function을 제외하고는 모두 연속적인 형태를 보여주었는데 이 ReLU는 0에서 미분할 수 없는 형태로 그래프가 그려집니다.

![temp](https://user-images.githubusercontent.com/91870042/174321928-559f89cd-b787-440f-a026-df87ceae0edb.png){: .align-center width="55%"}

$$
h(x) = max(x, 0)
$$

ReLU는 0 이하의 모든 값은 0으로 출력을 하고 그 이외의 값은 그대로 출력하는 함수입니다. 특히나 그 구조가 단순하고 0인 값들에 대해서는 계산의 필요성을 제거하기 때문에 연산량을 감소시킬 수 있습니다. 하지만 반대로, 이것이 독이 되어 신호를 전달하지 못하는 **Dying ReLU** 현상이 발생합니다. 추가로 ReLU를 사용하게 되면 양의 구간에 대해서는 기울기 소실문제를 해결할 수 있습니다. 그래도 여전히 음의 구간에서는 기울기 소실의 문제를 해결하지 못했습니다.😢

오차를 이용해서 학습하는 딥러닝의 경우 이 활성함수의 미분 값이 중요한데 ReLU는 0이 아닌 것이 대부분이기 때문에 빠른 학습이 가능하다는 장점도 존재합니다. 위의 ReLU를 미분하였을 때, 절반이 0이 되어서 이 말이 이상하게 느껴질 수 있습니다. 하지만, **학습 과정에서 다음 뉴런에 대부분 전달되는 값들은 0보다 크기 때문**에 학습이 잘 이루어질 수 있습니다.

ReLU는 학습에서 정말 중요한 활성함수이기에 개선하는 과정에서 나온 파생된 ReLU도 여러 개 존재합니다. 대표적으로 `Leaky ReLU, Parametric ReLU, ELU`등이 있습니다.

<br>

이렇게 해서 활성함수의 역할과 그 종류에 대해서 알아보았습니다. 다음 DL Basic 게시글에서는 신경망에서 학습을 진행하는 핵심인 오차역전파법에 대해서 말씀드리겠습니다🔥
